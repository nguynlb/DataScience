{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18587a44",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Clustering Text Document using KMeans Scikit Learn</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6423f46",
   "metadata": {},
   "source": [
    "This assigment follows the tutorial of [Scikit Learn](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de3c17",
   "metadata": {},
   "source": [
    "## 1. LoadData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b038d7",
   "metadata": {},
   "source": [
    "We load data from The 20 newsgroups text dataset, which comprises around 18,000 newsgroups posts on 20 topics. We select a subset of 4 topics only accounting for around 3,400 documents. See the example [Classification of text documents using sparse features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) to gain intuition on the overlap of such topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d21bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents - 4 categories\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "labels = dataset.target\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset.data)} documents - {true_k} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737ba8d",
   "metadata": {},
   "source": [
    "## 2. Quantifying the quantity of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d5cb1",
   "metadata": {},
   "source": [
    "In this section, we define a function to score different clustering evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da0157",
   "metadata": {},
   "source": [
    "Evaluation metrics are fundamentally clustering methods. \n",
    "If we have the class labels of a specific dataset, we can use this \"supervise\" ground truth method to quantify the quantity of results clustering.\n",
    "* homogeneity, which quantifies how much clusters contain only members of a single class;\n",
    "* completeness, which quantifies how much members of a given class are assigned to the same clusters;\n",
    "* V-measure, the harmonic mean of completeness and homogeneity;\n",
    "* Rand-Index, which measures how frequently pairs of data points are grouped consistently according to the result of the clustering algorithm and the ground truth class assignment;\n",
    "* Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster assignment have an ARI of 0.0 in expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981887d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "    \"\"\"\n",
    "    km: KMeans instance\n",
    "    X: Dataset\n",
    "    name: not been known yet\n",
    "    n_runs: Lưu trữ số lần train và mỗi lần train là một seed iteration chạy từ 0 -> n_runs\n",
    "    \"\"\"\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "    \n",
    "    train_times = []               # Time train of each seed(n_runs)\n",
    "    scores = defaultdict(list)     # Return a dict-like object. --> [(\"Bananas\": [1,2,3]) , (\"Apples\" : [2])]\n",
    "                                   # Dict store lists as items\n",
    "    for seed in range(n_runs):\n",
    "        # Set random_state parameter\n",
    "        km.set_params(random_state=seed) \n",
    "        \n",
    "        # Fit and store training time\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffe711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
