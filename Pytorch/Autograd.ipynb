{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "939b35ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\admin\\anaconda3\\envs\\dscoursera\\lib\\site-packages (0.20.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7d3070e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\backend\\execute.py:79\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[1;32m---> 79\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43m_run_input_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\backend\\execute.py:99\u001b[0m, in \u001b[0;36m_run_input_lines\u001b[1;34m(cmd, input_lines, kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_input_lines\u001b[39m(cmd, input_lines, \u001b[38;5;241m*\u001b[39m, kwargs):\n\u001b[1;32m---> 99\u001b[0m     popen \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd, stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     stdin_write \u001b[38;5;241m=\u001b[39m popen\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\IPython\\core\\formatters.py:974\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    971\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_mimebundle_\u001b[1;34m(self, include, exclude, **_)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name)()\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\jupyter_integration.py:112\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_image_svg_xml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_image_svg_xml\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the rendered graph as SVG string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msvg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSVG_ENCODING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\piping.py:104\u001b[0m, in \u001b[0;36mPipe.pipe\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m          \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m          renderer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m          engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m          encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the source piped through the Graphviz layout command.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m        '<?xml version='\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_legacy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\piping.py:121\u001b[0m, in \u001b[0;36mPipe._pipe_legacy\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@_tools\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecate_positional_args(supported_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe_legacy\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    114\u001b[0m                  \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m                  engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    120\u001b[0m                  encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\piping.py:149\u001b[0m, in \u001b[0;36mPipe._pipe_future\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(encoding) \u001b[38;5;129;01mis\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding):\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;66;03m# common case: both stdin and stdout need the same encoding\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines_string(\u001b[38;5;241m*\u001b[39margs, encoding\u001b[38;5;241m=\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines(\u001b[38;5;241m*\u001b[39margs, input_encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\backend\\piping.py:212\u001b[0m, in \u001b[0;36mpipe_lines_string\u001b[1;34m(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet)\u001b[0m\n\u001b[0;32m    206\u001b[0m cmd \u001b[38;5;241m=\u001b[39m dot_command\u001b[38;5;241m.\u001b[39mcommand(engine, \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    207\u001b[0m                           renderer\u001b[38;5;241m=\u001b[39mrenderer,\n\u001b[0;32m    208\u001b[0m                           formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[0;32m    209\u001b[0m                           neato_no_op\u001b[38;5;241m=\u001b[39mneato_no_op)\n\u001b[0;32m    210\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_lines\u001b[39m\u001b[38;5;124m'\u001b[39m: input_lines, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding}\n\u001b[1;32m--> 212\u001b[0m proc \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mrun_check(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, quiet\u001b[38;5;241m=\u001b[39mquiet, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DSCoursera\\lib\\site-packages\\graphviz\\backend\\execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x2042ce7fe80>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([3])\n",
    "y = torch.tensor([10])\n",
    "a = torch.tensor([1.], requires_grad=True)\n",
    "b = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "c = a * x\n",
    "y_hat = c + b\n",
    "z = y_hat - y\n",
    "L = z**2\n",
    "\n",
    "make_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afc16bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e01aa271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "590d8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySquare(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input,  = ctx.saved_tensors\n",
    "        return 2 * input * grad_output\n",
    "\n",
    "my_square = MySquare.apply\n",
    "\n",
    "\n",
    "x = torch.tensor([3])\n",
    "y = torch.tensor([10])\n",
    "a = torch.tensor([1.], requires_grad=True)\n",
    "b = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "y_hat = a*x + b\n",
    "z = y_hat - y\n",
    "L = my_square(z)\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92a7e886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-30.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67f86bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0c6f0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_linear.csv\").values\n",
    "X = torch.tensor(data[:, 0], dtype=torch.float64) / 10\n",
    "y = torch.tensor(data[:, 1], dtype=torch.float64) /500\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate = .01, max_iterations = 1000):\n",
    "        self.lr = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "        self.a = None\n",
    "        self.b = None\n",
    "        \n",
    "    def model(self, x):\n",
    "        return self.a * x + self.b\n",
    "    \n",
    "    def error(self, y_hat, y):\n",
    "        squared_diffs = (y_hat - y)**2\n",
    "        return squared_diffs.mean()\n",
    "\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.a = torch.tensor(0.5, requires_grad = True, dtype=torch.float64)\n",
    "        self.b = torch.tensor(0.5, requires_grad = True, dtype=torch.float64)\n",
    "        \n",
    "        for ite in range(1, self.max_iterations + 1):\n",
    "        # nếu có grad ở tham số a, b thì zero đi, tránh trường hợp cộng dồn grad\n",
    "            if self.a.grad is not None:\n",
    "                self.a.grad.zero_()\n",
    "            if self.b.grad is not None:\n",
    "                self.b.grad.zero_()\n",
    "        \n",
    "            # xây model, loss\n",
    "            y_hat = self.model(x)\n",
    "            loss = self.error(y_hat, y)\n",
    "\n",
    "            # gọi backward để tính đạo hàm ngược của loss với tham số a, b\n",
    "            loss.backward()\n",
    "\n",
    "            # update a,b bằng thuật toán gradient descent, để torch.no_grad thì mình không cần backward ở bước này\n",
    "            with torch.no_grad():\n",
    "                self.a -= self.lr * self.a.grad\n",
    "                self.b -= self.lr * self.b.grad\n",
    "            print(f\"Epoch {ite}, loss metrics {loss}\")       \n",
    "            \n",
    "    def predict(self, X):\n",
    "        return self.a * X + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0dfc0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "86da9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss metrics 3.3605401400941997\n",
      "Epoch 2, loss metrics 0.030447082039855133\n",
      "Epoch 3, loss metrics 0.022197049655934616\n",
      "Epoch 4, loss metrics 0.02209921413266073\n",
      "Epoch 5, loss metrics 0.022021669165024224\n",
      "Epoch 6, loss metrics 0.021944457744633942\n",
      "Epoch 7, loss metrics 0.021867529854801625\n",
      "Epoch 8, loss metrics 0.02179088433506812\n",
      "Epoch 9, loss metrics 0.021714520148678237\n",
      "Epoch 10, loss metrics 0.021638436262975298\n",
      "Epoch 11, loss metrics 0.021562631649093816\n",
      "Epoch 12, loss metrics 0.021487105281944908\n",
      "Epoch 13, loss metrics 0.02141185614020222\n",
      "Epoch 14, loss metrics 0.021336883206288362\n",
      "Epoch 15, loss metrics 0.02126218546636101\n",
      "Epoch 16, loss metrics 0.021187761910299235\n",
      "Epoch 17, loss metrics 0.021113611531689825\n",
      "Epoch 18, loss metrics 0.021039733327813684\n",
      "Epoch 19, loss metrics 0.020966126299632248\n",
      "Epoch 20, loss metrics 0.02089278945177409\n",
      "Epoch 21, loss metrics 0.020819721792521277\n",
      "Epoch 22, loss metrics 0.020746922333796113\n",
      "Epoch 23, loss metrics 0.020674390091147694\n",
      "Epoch 24, loss metrics 0.020602124083738616\n",
      "Epoch 25, loss metrics 0.02053012333433169\n",
      "Epoch 26, loss metrics 0.020458386869276843\n",
      "Epoch 27, loss metrics 0.020386913718497764\n",
      "Epoch 28, loss metrics 0.02031570291547893\n",
      "Epoch 29, loss metrics 0.02024475349725247\n",
      "Epoch 30, loss metrics 0.020174064504385168\n",
      "Epoch 31, loss metrics 0.020103634980965474\n",
      "Epoch 32, loss metrics 0.020033463974590635\n",
      "Epoch 33, loss metrics 0.019963550536353725\n",
      "Epoch 34, loss metrics 0.01989389372083078\n",
      "Epoch 35, loss metrics 0.019824492586068238\n",
      "Epoch 36, loss metrics 0.019755346193569962\n",
      "Epoch 37, loss metrics 0.01968645360828464\n",
      "Epoch 38, loss metrics 0.01961781389859313\n",
      "Epoch 39, loss metrics 0.019549426136295955\n",
      "Epoch 40, loss metrics 0.01948128939660053\n",
      "Epoch 41, loss metrics 0.019413402758108968\n",
      "Epoch 42, loss metrics 0.019345765302805216\n",
      "Epoch 43, loss metrics 0.019278376116043067\n",
      "Epoch 44, loss metrics 0.019211234286533565\n",
      "Epoch 45, loss metrics 0.01914433890633259\n",
      "Epoch 46, loss metrics 0.019077689070828838\n",
      "Epoch 47, loss metrics 0.019011283878731345\n",
      "Epoch 48, loss metrics 0.018945122432057537\n",
      "Epoch 49, loss metrics 0.018879203836120805\n",
      "Epoch 50, loss metrics 0.018813527199518607\n",
      "Epoch 51, loss metrics 0.01874809163412044\n",
      "Epoch 52, loss metrics 0.018682896255055714\n",
      "Epoch 53, loss metrics 0.0186179401807017\n",
      "Epoch 54, loss metrics 0.018553222532671967\n",
      "Epoch 55, loss metrics 0.018488742435804048\n",
      "Epoch 56, loss metrics 0.018424499018147984\n",
      "Epoch 57, loss metrics 0.01836049141095427\n",
      "Epoch 58, loss metrics 0.018296718748662306\n",
      "Epoch 59, loss metrics 0.018233180168888488\n",
      "Epoch 60, loss metrics 0.0181698748124148\n",
      "Epoch 61, loss metrics 0.018106801823176918\n",
      "Epoch 62, loss metrics 0.01804396034825284\n",
      "Epoch 63, loss metrics 0.01798134953785128\n",
      "Epoch 64, loss metrics 0.017918968545300135\n",
      "Epoch 65, loss metrics 0.017856816527035135\n",
      "Epoch 66, loss metrics 0.017794892642588347\n",
      "Epoch 67, loss metrics 0.017733196054576832\n",
      "Epoch 68, loss metrics 0.017671725928691328\n",
      "Epoch 69, loss metrics 0.01761048143368503\n",
      "Epoch 70, loss metrics 0.017549461741362186\n",
      "Epoch 71, loss metrics 0.017488666026567092\n",
      "Epoch 72, loss metrics 0.017428093467172792\n",
      "Epoch 73, loss metrics 0.017367743244070078\n",
      "Epoch 74, loss metrics 0.017307614541156216\n",
      "Epoch 75, loss metrics 0.017247706545324155\n",
      "Epoch 76, loss metrics 0.017188018446451403\n",
      "Epoch 77, loss metrics 0.01712854943738903\n",
      "Epoch 78, loss metrics 0.01706929871395086\n",
      "Epoch 79, loss metrics 0.017010265474902506\n",
      "Epoch 80, loss metrics 0.016951448921950624\n",
      "Epoch 81, loss metrics 0.01689284825973204\n",
      "Epoch 82, loss metrics 0.016834462695802997\n",
      "Epoch 83, loss metrics 0.0167762914406285\n",
      "Epoch 84, loss metrics 0.016718333707571618\n",
      "Epoch 85, loss metrics 0.016660588712882788\n",
      "Epoch 86, loss metrics 0.016603055675689246\n",
      "Epoch 87, loss metrics 0.01654573381798458\n",
      "Epoch 88, loss metrics 0.016488622364617986\n",
      "Epoch 89, loss metrics 0.01643172054328404\n",
      "Epoch 90, loss metrics 0.016375027584511995\n",
      "Epoch 91, loss metrics 0.01631854272165558\n",
      "Epoch 92, loss metrics 0.01626226519088258\n",
      "Epoch 93, loss metrics 0.016206194231164414\n",
      "Epoch 94, loss metrics 0.016150329084265995\n",
      "Epoch 95, loss metrics 0.016094668994735335\n",
      "Epoch 96, loss metrics 0.016039213209893426\n",
      "Epoch 97, loss metrics 0.015983960979823995\n",
      "Epoch 98, loss metrics 0.015928911557363413\n",
      "Epoch 99, loss metrics 0.01587406419809059\n",
      "Epoch 100, loss metrics 0.015819418160316876\n",
      "Epoch 101, loss metrics 0.01576497270507601\n",
      "Epoch 102, loss metrics 0.015710727096114263\n",
      "Epoch 103, loss metrics 0.015656680599880252\n",
      "Epoch 104, loss metrics 0.015602832485515286\n",
      "Epoch 105, loss metrics 0.01554918202484324\n",
      "Epoch 106, loss metrics 0.015495728492360868\n",
      "Epoch 107, loss metrics 0.015442471165227921\n",
      "Epoch 108, loss metrics 0.015389409323257417\n",
      "Epoch 109, loss metrics 0.015336542248905882\n",
      "Epoch 110, loss metrics 0.015283869227263545\n",
      "Epoch 111, loss metrics 0.015231389546044905\n",
      "Epoch 112, loss metrics 0.015179102495578852\n",
      "Epoch 113, loss metrics 0.015127007368799258\n",
      "Epoch 114, loss metrics 0.015075103461235303\n",
      "Epoch 115, loss metrics 0.01502339007100193\n",
      "Epoch 116, loss metrics 0.014971866498790508\n",
      "Epoch 117, loss metrics 0.014920532047859205\n",
      "Epoch 118, loss metrics 0.014869386024023634\n",
      "Epoch 119, loss metrics 0.014818427735647507\n",
      "Epoch 120, loss metrics 0.01476765649363319\n",
      "Epoch 121, loss metrics 0.014717071611412506\n",
      "Epoch 122, loss metrics 0.01466667240493724\n",
      "Epoch 123, loss metrics 0.014616458192670245\n",
      "Epoch 124, loss metrics 0.014566428295575798\n",
      "Epoch 125, loss metrics 0.014516582037110766\n",
      "Epoch 126, loss metrics 0.014466918743215233\n",
      "Epoch 127, loss metrics 0.01441743774230354\n",
      "Epoch 128, loss metrics 0.01436813836525513\n",
      "Epoch 129, loss metrics 0.014319019945405439\n",
      "Epoch 130, loss metrics 0.014270081818537025\n",
      "Epoch 131, loss metrics 0.014221323322870408\n",
      "Epoch 132, loss metrics 0.014172743799055388\n",
      "Epoch 133, loss metrics 0.014124342590161822\n",
      "Epoch 134, loss metrics 0.01407611904167087\n",
      "Epoch 135, loss metrics 0.014028072501466265\n",
      "Epoch 136, loss metrics 0.013980202319825271\n",
      "Epoch 137, loss metrics 0.013932507849410107\n",
      "Epoch 138, loss metrics 0.01388498844525903\n",
      "Epoch 139, loss metrics 0.01383764346477769\n",
      "Epoch 140, loss metrics 0.01379047226773045\n",
      "Epoch 141, loss metrics 0.01374347421623167\n",
      "Epoch 142, loss metrics 0.013696648674737199\n",
      "Epoch 143, loss metrics 0.013649995010035562\n",
      "Epoch 144, loss metrics 0.013603512591239708\n",
      "Epoch 145, loss metrics 0.013557200789778184\n",
      "Epoch 146, loss metrics 0.013511058979386738\n",
      "Epoch 147, loss metrics 0.01346508653610002\n",
      "Epoch 148, loss metrics 0.013419282838242829\n",
      "Epoch 149, loss metrics 0.013373647266421955\n",
      "Epoch 150, loss metrics 0.013328179203517695\n",
      "Epoch 151, loss metrics 0.013282878034675582\n",
      "Epoch 152, loss metrics 0.013237743147297877\n",
      "Epoch 153, loss metrics 0.013192773931035656\n",
      "Epoch 154, loss metrics 0.013147969777780115\n",
      "Epoch 155, loss metrics 0.013103330081654653\n",
      "Epoch 156, loss metrics 0.013058854239006608\n",
      "Epoch 157, loss metrics 0.013014541648399039\n",
      "Epoch 158, loss metrics 0.012970391710602618\n",
      "Epoch 159, loss metrics 0.012926403828587578\n",
      "Epoch 160, loss metrics 0.012882577407515533\n",
      "Epoch 161, loss metrics 0.01283891185473152\n",
      "Epoch 162, loss metrics 0.012795406579756023\n",
      "Epoch 163, loss metrics 0.01275206099427685\n",
      "Epoch 164, loss metrics 0.012708874512141274\n",
      "Epoch 165, loss metrics 0.012665846549348077\n",
      "Epoch 166, loss metrics 0.012622976524039726\n",
      "Epoch 167, loss metrics 0.012580263856494352\n",
      "Epoch 168, loss metrics 0.012537707969118102\n",
      "Epoch 169, loss metrics 0.012495308286437112\n",
      "Epoch 170, loss metrics 0.01245306423508994\n",
      "Epoch 171, loss metrics 0.012410975243819618\n",
      "Epoch 172, loss metrics 0.012369040743466104\n",
      "Epoch 173, loss metrics 0.012327260166958464\n",
      "Epoch 174, loss metrics 0.012285632949307256\n",
      "Epoch 175, loss metrics 0.01224415852759687\n",
      "Epoch 176, loss metrics 0.012202836340977878\n",
      "Epoch 177, loss metrics 0.01216166583065955\n",
      "Epoch 178, loss metrics 0.01212064643990225\n",
      "Epoch 179, loss metrics 0.012079777614009872\n",
      "Epoch 180, loss metrics 0.012039058800322348\n",
      "Epoch 181, loss metrics 0.011998489448208196\n",
      "Epoch 182, loss metrics 0.01195806900905712\n",
      "Epoch 183, loss metrics 0.011917796936272474\n",
      "Epoch 184, loss metrics 0.01187767268526398\n",
      "Epoch 185, loss metrics 0.011837695713440309\n",
      "Epoch 186, loss metrics 0.011797865480201754\n",
      "Epoch 187, loss metrics 0.01175818144693295\n",
      "Epoch 188, loss metrics 0.011718643076995484\n",
      "Epoch 189, loss metrics 0.011679249835720785\n",
      "Epoch 190, loss metrics 0.011640001190402812\n",
      "Epoch 191, loss metrics 0.01160089661029088\n",
      "Epoch 192, loss metrics 0.011561935566582391\n",
      "Epoch 193, loss metrics 0.01152311753241585\n",
      "Epoch 194, loss metrics 0.01148444198286359\n",
      "Epoch 195, loss metrics 0.011445908394924764\n",
      "Epoch 196, loss metrics 0.011407516247518215\n",
      "Epoch 197, loss metrics 0.011369265021475474\n",
      "Epoch 198, loss metrics 0.011331154199533679\n",
      "Epoch 199, loss metrics 0.0112931832663287\n",
      "Epoch 200, loss metrics 0.011255351708388012\n",
      "Epoch 201, loss metrics 0.011217659014123893\n",
      "Epoch 202, loss metrics 0.011180104673826389\n",
      "Epoch 203, loss metrics 0.011142688179656534\n",
      "Epoch 204, loss metrics 0.011105409025639359\n",
      "Epoch 205, loss metrics 0.011068266707657152\n",
      "Epoch 206, loss metrics 0.011031260723442654\n",
      "Epoch 207, loss metrics 0.010994390572572083\n",
      "Epoch 208, loss metrics 0.010957655756458613\n",
      "Epoch 209, loss metrics 0.010921055778345538\n",
      "Epoch 210, loss metrics 0.010884590143299422\n",
      "Epoch 211, loss metrics 0.010848258358203633\n",
      "Epoch 212, loss metrics 0.010812059931751498\n",
      "Epoch 213, loss metrics 0.01077599437443973\n",
      "Epoch 214, loss metrics 0.010740061198561805\n",
      "Epoch 215, loss metrics 0.010704259918201406\n",
      "Epoch 216, loss metrics 0.010668590049225771\n",
      "Epoch 217, loss metrics 0.010633051109279158\n",
      "Epoch 218, loss metrics 0.010597642617776397\n",
      "Epoch 219, loss metrics 0.010562364095896344\n",
      "Epoch 220, loss metrics 0.010527215066575352\n",
      "Epoch 221, loss metrics 0.010492195054500905\n",
      "Epoch 222, loss metrics 0.010457303586105226\n",
      "Epoch 223, loss metrics 0.010422540189558686\n",
      "Epoch 224, loss metrics 0.010387904394763607\n",
      "Epoch 225, loss metrics 0.01035339573334786\n",
      "Epoch 226, loss metrics 0.010319013738658433\n",
      "Epoch 227, loss metrics 0.010284757945755323\n",
      "Epoch 228, loss metrics 0.010250627891404988\n",
      "Epoch 229, loss metrics 0.01021662311407432\n",
      "Epoch 230, loss metrics 0.010182743153924303\n",
      "Epoch 231, loss metrics 0.010148987552803699\n",
      "Epoch 232, loss metrics 0.010115355854243066\n",
      "Epoch 233, loss metrics 0.01008184760344841\n",
      "Epoch 234, loss metrics 0.010048462347295086\n",
      "Epoch 235, loss metrics 0.010015199634321693\n",
      "Epoch 236, loss metrics 0.009982059014723977\n",
      "Epoch 237, loss metrics 0.009949040040348706\n",
      "Epoch 238, loss metrics 0.009916142264687611\n",
      "Epoch 239, loss metrics 0.009883365242871365\n",
      "Epoch 240, loss metrics 0.00985070853166362\n",
      "Epoch 241, loss metrics 0.009818171689454924\n",
      "Epoch 242, loss metrics 0.009785754276256758\n",
      "Epoch 243, loss metrics 0.009753455853695686\n",
      "Epoch 244, loss metrics 0.009721275985007317\n",
      "Epoch 245, loss metrics 0.009689214235030412\n",
      "Epoch 246, loss metrics 0.00965727017020107\n",
      "Epoch 247, loss metrics 0.009625443358546804\n",
      "Epoch 248, loss metrics 0.009593733369680684\n",
      "Epoch 249, loss metrics 0.009562139774795596\n",
      "Epoch 250, loss metrics 0.009530662146658424\n",
      "Epoch 251, loss metrics 0.009499300059604117\n",
      "Epoch 252, loss metrics 0.009468053089530182\n",
      "Epoch 253, loss metrics 0.009436920813890793\n",
      "Epoch 254, loss metrics 0.0094059028116911\n",
      "Epoch 255, loss metrics 0.009374998663481546\n",
      "Epoch 256, loss metrics 0.00934420795135221\n",
      "Epoch 257, loss metrics 0.009313530258927116\n",
      "Epoch 258, loss metrics 0.009282965171358676\n",
      "Epoch 259, loss metrics 0.009252512275321983\n",
      "Epoch 260, loss metrics 0.009222171159009305\n",
      "Epoch 261, loss metrics 0.009191941412124467\n",
      "Epoch 262, loss metrics 0.009161822625877286\n",
      "Epoch 263, loss metrics 0.009131814392978176\n",
      "Epoch 264, loss metrics 0.00910191630763241\n",
      "Epoch 265, loss metrics 0.009072127965534863\n",
      "Epoch 266, loss metrics 0.009042448963864393\n",
      "Epoch 267, loss metrics 0.009012878901278418\n",
      "Epoch 268, loss metrics 0.008983417377907638\n",
      "Epoch 269, loss metrics 0.00895406399535034\n",
      "Epoch 270, loss metrics 0.008924818356667304\n",
      "Epoch 271, loss metrics 0.008895680066376232\n",
      "Epoch 272, loss metrics 0.008866648730446514\n",
      "Epoch 273, loss metrics 0.008837723956293843\n",
      "Epoch 274, loss metrics 0.008808905352774928\n",
      "Epoch 275, loss metrics 0.008780192530182149\n",
      "Epoch 276, loss metrics 0.00875158510023848\n",
      "Epoch 277, loss metrics 0.00872308267609193\n",
      "Epoch 278, loss metrics 0.00869468487231061\n",
      "Epoch 279, loss metrics 0.00866639130487726\n",
      "Epoch 280, loss metrics 0.008638201591184313\n",
      "Epoch 281, loss metrics 0.008610115350028504\n",
      "Epoch 282, loss metrics 0.00858213220160586\n",
      "Epoch 283, loss metrics 0.00855425176750647\n",
      "Epoch 284, loss metrics 0.008526473670709427\n",
      "Epoch 285, loss metrics 0.00849879753557767\n",
      "Epoch 286, loss metrics 0.00847122298785302\n",
      "Epoch 287, loss metrics 0.008443749654650962\n",
      "Epoch 288, loss metrics 0.008416377164455747\n",
      "Epoch 289, loss metrics 0.008389105147115241\n",
      "Epoch 290, loss metrics 0.00836193323383604\n",
      "Epoch 291, loss metrics 0.00833486105717837\n",
      "Epoch 292, loss metrics 0.00830788825105125\n",
      "Epoch 293, loss metrics 0.00828101445070737\n",
      "Epoch 294, loss metrics 0.008254239292738328\n",
      "Epoch 295, loss metrics 0.008227562415069613\n",
      "Epoch 296, loss metrics 0.008200983456955713\n",
      "Epoch 297, loss metrics 0.008174502058975316\n",
      "Epoch 298, loss metrics 0.008148117863026324\n",
      "Epoch 299, loss metrics 0.008121830512321142\n",
      "Epoch 300, loss metrics 0.008095639651381785\n",
      "Epoch 301, loss metrics 0.00806954492603503\n",
      "Epoch 302, loss metrics 0.00804354598340772\n",
      "Epoch 303, loss metrics 0.008017642471921944\n",
      "Epoch 304, loss metrics 0.007991834041290244\n",
      "Epoch 305, loss metrics 0.007966120342510982\n",
      "Epoch 306, loss metrics 0.00794050102786348\n",
      "Epoch 307, loss metrics 0.007914975750903481\n",
      "Epoch 308, loss metrics 0.007889544166458314\n",
      "Epoch 309, loss metrics 0.007864205930622325\n",
      "Epoch 310, loss metrics 0.007838960700752183\n",
      "Epoch 311, loss metrics 0.007813808135462254\n",
      "Epoch 312, loss metrics 0.007788747894619976\n",
      "Epoch 313, loss metrics 0.007763779639341276\n",
      "Epoch 314, loss metrics 0.007738903031986028\n",
      "Epoch 315, loss metrics 0.007714117736153364\n",
      "Epoch 316, loss metrics 0.007689423416677234\n",
      "Epoch 317, loss metrics 0.007664819739621861\n",
      "Epoch 318, loss metrics 0.007640306372277171\n",
      "Epoch 319, loss metrics 0.00761588298315436\n",
      "Epoch 320, loss metrics 0.007591549241981369\n",
      "Epoch 321, loss metrics 0.007567304819698405\n",
      "Epoch 322, loss metrics 0.0075431493884535435\n",
      "Epoch 323, loss metrics 0.007519082621598272\n",
      "Epoch 324, loss metrics 0.007495104193683006\n",
      "Epoch 325, loss metrics 0.007471213780452847\n",
      "Epoch 326, loss metrics 0.0074474110588430105\n",
      "Epoch 327, loss metrics 0.007423695706974598\n",
      "Epoch 328, loss metrics 0.007400067404150168\n",
      "Epoch 329, loss metrics 0.007376525830849436\n",
      "Epoch 330, loss metrics 0.007353070668724958\n",
      "Epoch 331, loss metrics 0.007329701600597723\n",
      "Epoch 332, loss metrics 0.0073064183104531\n",
      "Epoch 333, loss metrics 0.0072832204834362806\n",
      "Epoch 334, loss metrics 0.007260107805848243\n",
      "Epoch 335, loss metrics 0.007237079965141331\n",
      "Epoch 336, loss metrics 0.007214136649915211\n",
      "Epoch 337, loss metrics 0.007191277549912524\n",
      "Epoch 338, loss metrics 0.007168502356014729\n",
      "Epoch 339, loss metrics 0.007145810760237962\n",
      "Epoch 340, loss metrics 0.00712320245572878\n",
      "Epoch 341, loss metrics 0.007100677136760163\n",
      "Epoch 342, loss metrics 0.007078234498727152\n",
      "Epoch 343, loss metrics 0.0070558742381430185\n",
      "Epoch 344, loss metrics 0.007033596052634877\n",
      "Epoch 345, loss metrics 0.007011399640939795\n",
      "Epoch 346, loss metrics 0.006989284702900606\n",
      "Epoch 347, loss metrics 0.006967250939461963\n",
      "Epoch 348, loss metrics 0.006945298052666162\n",
      "Epoch 349, loss metrics 0.006923425745649164\n",
      "Epoch 350, loss metrics 0.006901633722636644\n",
      "Epoch 351, loss metrics 0.006879921688939926\n",
      "Epoch 352, loss metrics 0.006858289350951993\n",
      "Epoch 353, loss metrics 0.006836736416143555\n",
      "Epoch 354, loss metrics 0.006815262593059053\n",
      "Epoch 355, loss metrics 0.0067938675913127656\n",
      "Epoch 356, loss metrics 0.006772551121584813\n",
      "Epoch 357, loss metrics 0.006751312895617324\n",
      "Epoch 358, loss metrics 0.006730152626210504\n",
      "Epoch 359, loss metrics 0.006709070027218708\n",
      "Epoch 360, loss metrics 0.006688064813546676\n",
      "Epoch 361, loss metrics 0.0066671367011455155\n",
      "Epoch 362, loss metrics 0.006646285407009056\n",
      "Epoch 363, loss metrics 0.006625510649169848\n",
      "Epoch 364, loss metrics 0.006604812146695476\n",
      "Epoch 365, loss metrics 0.006584189619684719\n",
      "Epoch 366, loss metrics 0.006563642789263668\n",
      "Epoch 367, loss metrics 0.006543171377582163\n",
      "Epoch 368, loss metrics 0.0065227751078098025\n",
      "Epoch 369, loss metrics 0.0065024537041324\n",
      "Epoch 370, loss metrics 0.006482206891748107\n",
      "Epoch 371, loss metrics 0.006462034396863783\n",
      "Epoch 372, loss metrics 0.006441935946691265\n",
      "Epoch 373, loss metrics 0.006421911269443676\n",
      "Epoch 374, loss metrics 0.006401960094331777\n",
      "Epoch 375, loss metrics 0.006382082151560226\n",
      "Epoch 376, loss metrics 0.0063622771723240296\n",
      "Epoch 377, loss metrics 0.006342544888804877\n",
      "Epoch 378, loss metrics 0.006322885034167471\n",
      "Epoch 379, loss metrics 0.006303297342555956\n",
      "Epoch 380, loss metrics 0.006283781549090348\n",
      "Epoch 381, loss metrics 0.0062643373898628885\n",
      "Epoch 382, loss metrics 0.006244964601934551\n",
      "Epoch 383, loss metrics 0.0062256629233314355\n",
      "Epoch 384, loss metrics 0.0062064320930412175\n",
      "Epoch 385, loss metrics 0.006187271851009662\n",
      "Epoch 386, loss metrics 0.00616818193813705\n",
      "Epoch 387, loss metrics 0.006149162096274758\n",
      "Epoch 388, loss metrics 0.006130212068221687\n",
      "Epoch 389, loss metrics 0.00611133159772082\n",
      "Epoch 390, loss metrics 0.006092520429455741\n",
      "Epoch 391, loss metrics 0.006073778309047201\n",
      "Epoch 392, loss metrics 0.006055104983049677\n",
      "Epoch 393, loss metrics 0.00603650019894796\n",
      "Epoch 394, loss metrics 0.006017963705153645\n",
      "Epoch 395, loss metrics 0.005999495251001838\n",
      "Epoch 396, loss metrics 0.005981094586747785\n",
      "Epoch 397, loss metrics 0.005962761463563376\n",
      "Epoch 398, loss metrics 0.005944495633533846\n",
      "Epoch 399, loss metrics 0.005926296849654446\n",
      "Epoch 400, loss metrics 0.005908164865827059\n",
      "Epoch 401, loss metrics 0.005890099436856899\n",
      "Epoch 402, loss metrics 0.005872100318449192\n",
      "Epoch 403, loss metrics 0.005854167267205857\n",
      "Epoch 404, loss metrics 0.005836300040622225\n",
      "Epoch 405, loss metrics 0.00581849839708376\n",
      "Epoch 406, loss metrics 0.005800762095862796\n",
      "Epoch 407, loss metrics 0.005783090897115266\n",
      "Epoch 408, loss metrics 0.005765484561877484\n",
      "Epoch 409, loss metrics 0.0057479428520629035\n",
      "Epoch 410, loss metrics 0.0057304655304588606\n",
      "Epoch 411, loss metrics 0.005713052360723418\n",
      "Epoch 412, loss metrics 0.005695703107382172\n",
      "Epoch 413, loss metrics 0.005678417535825013\n",
      "Epoch 414, loss metrics 0.0056611954123029756\n",
      "Epoch 415, loss metrics 0.005644036503925148\n",
      "Epoch 416, loss metrics 0.00562694057865542\n",
      "Epoch 417, loss metrics 0.005609907405309393\n",
      "Epoch 418, loss metrics 0.005592936753551219\n",
      "Epoch 419, loss metrics 0.005576028393890568\n",
      "Epoch 420, loss metrics 0.005559182097679464\n",
      "Epoch 421, loss metrics 0.00554239763710915\n",
      "Epoch 422, loss metrics 0.005525674785207139\n",
      "Epoch 423, loss metrics 0.005509013315833995\n",
      "Epoch 424, loss metrics 0.005492413003680389\n",
      "Epoch 425, loss metrics 0.005475873624263958\n",
      "Epoch 426, loss metrics 0.005459394953926388\n",
      "Epoch 427, loss metrics 0.005442976769830233\n",
      "Epoch 428, loss metrics 0.0054266188499561005\n",
      "Epoch 429, loss metrics 0.005410320973099429\n",
      "Epoch 430, loss metrics 0.0053940829188676535\n",
      "Epoch 431, loss metrics 0.005377904467677206\n",
      "Epoch 432, loss metrics 0.005361785400750456\n",
      "Epoch 433, loss metrics 0.005345725500112845\n",
      "Epoch 434, loss metrics 0.00532972454858992\n",
      "Epoch 435, loss metrics 0.005313782329804384\n",
      "Epoch 436, loss metrics 0.005297898628173126\n",
      "Epoch 437, loss metrics 0.005282073228904392\n",
      "Epoch 438, loss metrics 0.0052663059179948275\n",
      "Epoch 439, loss metrics 0.005250596482226579\n",
      "Epoch 440, loss metrics 0.005234944709164442\n",
      "Epoch 441, loss metrics 0.005219350387152974\n",
      "Epoch 442, loss metrics 0.0052038133053136275\n",
      "Epoch 443, loss metrics 0.005188333253541868\n",
      "Epoch 444, loss metrics 0.0051729100225044585\n",
      "Epoch 445, loss metrics 0.005157543403636389\n",
      "Epoch 446, loss metrics 0.0051422331891383686\n",
      "Epoch 447, loss metrics 0.005126979171973712\n",
      "Epoch 448, loss metrics 0.005111781145865754\n",
      "Epoch 449, loss metrics 0.005096638905294988\n",
      "Epoch 450, loss metrics 0.005081552245496228\n",
      "Epoch 451, loss metrics 0.00506652096245594\n",
      "Epoch 452, loss metrics 0.005051544852909413\n",
      "Epoch 453, loss metrics 0.0050366237143380415\n",
      "Epoch 454, loss metrics 0.005021757344966617\n",
      "Epoch 455, loss metrics 0.005006945543760523\n",
      "Epoch 456, loss metrics 0.004992188110423016\n",
      "Epoch 457, loss metrics 0.004977484845392673\n",
      "Epoch 458, loss metrics 0.004962835549840466\n",
      "Epoch 459, loss metrics 0.004948240025667216\n",
      "Epoch 460, loss metrics 0.00493369807550094\n",
      "Epoch 461, loss metrics 0.004919209502694023\n",
      "Epoch 462, loss metrics 0.004904774111320755\n",
      "Epoch 463, loss metrics 0.004890391706174561\n",
      "Epoch 464, loss metrics 0.004876062092765313\n",
      "Epoch 465, loss metrics 0.00486178507731689\n",
      "Epoch 466, loss metrics 0.004847560466764347\n",
      "Epoch 467, loss metrics 0.0048333880687514395\n",
      "Epoch 468, loss metrics 0.004819267691628022\n",
      "Epoch 469, loss metrics 0.0048051991444473165\n",
      "Epoch 470, loss metrics 0.004791182236963505\n",
      "Epoch 471, loss metrics 0.004777216779629071\n",
      "Epoch 472, loss metrics 0.004763302583592245\n",
      "Epoch 473, loss metrics 0.004749439460694462\n",
      "Epoch 474, loss metrics 0.004735627223467733\n",
      "Epoch 475, loss metrics 0.004721865685132279\n",
      "Epoch 476, loss metrics 0.004708154659593898\n",
      "Epoch 477, loss metrics 0.004694493961441393\n",
      "Epoch 478, loss metrics 0.004680883405944197\n",
      "Epoch 479, loss metrics 0.00466732280904981\n",
      "Epoch 480, loss metrics 0.00465381198738129\n",
      "Epoch 481, loss metrics 0.004640350758234776\n",
      "Epoch 482, loss metrics 0.004626938939577072\n",
      "Epoch 483, loss metrics 0.004613576350043131\n",
      "Epoch 484, loss metrics 0.004600262808933637\n",
      "Epoch 485, loss metrics 0.00458699813621251\n",
      "Epoch 486, loss metrics 0.004573782152504532\n",
      "Epoch 487, loss metrics 0.004560614679092912\n",
      "Epoch 488, loss metrics 0.004547495537916827\n",
      "Epoch 489, loss metrics 0.004534424551569046\n",
      "Epoch 490, loss metrics 0.004521401543293511\n",
      "Epoch 491, loss metrics 0.004508426336983003\n",
      "Epoch 492, loss metrics 0.004495498757176664\n",
      "Epoch 493, loss metrics 0.004482618629057699\n",
      "Epoch 494, loss metrics 0.004469785778451028\n",
      "Epoch 495, loss metrics 0.00445700003182084\n",
      "Epoch 496, loss metrics 0.004444261216268356\n",
      "Epoch 497, loss metrics 0.004431569159529371\n",
      "Epoch 498, loss metrics 0.004418923689972032\n",
      "Epoch 499, loss metrics 0.004406324636594464\n",
      "Epoch 500, loss metrics 0.0043937718290224794\n",
      "Epoch 501, loss metrics 0.004381265097507233\n",
      "Epoch 502, loss metrics 0.004368804272922988\n",
      "Epoch 503, loss metrics 0.004356389186764785\n",
      "Epoch 504, loss metrics 0.004344019671146154\n",
      "Epoch 505, loss metrics 0.00433169555879691\n",
      "Epoch 506, loss metrics 0.004319416683060799\n",
      "Epoch 507, loss metrics 0.0043071828778933315\n",
      "Epoch 508, loss metrics 0.004294993977859459\n",
      "Epoch 509, loss metrics 0.004282849818131405\n",
      "Epoch 510, loss metrics 0.004270750234486402\n",
      "Epoch 511, loss metrics 0.004258695063304451\n",
      "Epoch 512, loss metrics 0.004246684141566184\n",
      "Epoch 513, loss metrics 0.004234717306850558\n",
      "Epoch 514, loss metrics 0.0042227943973327095\n",
      "Epoch 515, loss metrics 0.00421091525178179\n",
      "Epoch 516, loss metrics 0.004199079709558779\n",
      "Epoch 517, loss metrics 0.004187287610614235\n",
      "Epoch 518, loss metrics 0.004175538795486265\n",
      "Epoch 519, loss metrics 0.004163833105298214\n",
      "Epoch 520, loss metrics 0.004152170381756655\n",
      "Epoch 521, loss metrics 0.0041405504671491655\n",
      "Epoch 522, loss metrics 0.004128973204342227\n",
      "Epoch 523, loss metrics 0.004117438436779089\n",
      "Epoch 524, loss metrics 0.004105946008477637\n",
      "Epoch 525, loss metrics 0.004094495764028314\n",
      "Epoch 526, loss metrics 0.004083087548592047\n",
      "Epoch 527, loss metrics 0.00407172120789804\n",
      "Epoch 528, loss metrics 0.0040603965882418\n",
      "Epoch 529, loss metrics 0.0040491135364830195\n",
      "Epoch 530, loss metrics 0.004037871900043483\n",
      "Epoch 531, loss metrics 0.004026671526905056\n",
      "Epoch 532, loss metrics 0.004015512265607584\n",
      "Epoch 533, loss metrics 0.004004393965246847\n",
      "Epoch 534, loss metrics 0.003993316475472523\n",
      "Epoch 535, loss metrics 0.003982279646486217\n",
      "Epoch 536, loss metrics 0.003971283329039313\n",
      "Epoch 537, loss metrics 0.00396032737443106\n",
      "Epoch 538, loss metrics 0.003949411634506527\n",
      "Epoch 539, loss metrics 0.003938535961654581\n",
      "Epoch 540, loss metrics 0.003927700208805921\n",
      "Epoch 541, loss metrics 0.003916904229431088\n",
      "Epoch 542, loss metrics 0.003906147877538411\n",
      "Epoch 543, loss metrics 0.003895431007672162\n",
      "Epoch 544, loss metrics 0.003884753474910469\n",
      "Epoch 545, loss metrics 0.00387411513486346\n",
      "Epoch 546, loss metrics 0.0038635158436712076\n",
      "Epoch 547, loss metrics 0.0038529554580018573\n",
      "Epoch 548, loss metrics 0.0038424338350496454\n",
      "Epoch 549, loss metrics 0.0038319508325330375\n",
      "Epoch 550, loss metrics 0.0038215063086926753\n",
      "Epoch 551, loss metrics 0.0038111001222896276\n",
      "Epoch 552, loss metrics 0.003800732132603329\n",
      "Epoch 553, loss metrics 0.003790402199429748\n",
      "Epoch 554, loss metrics 0.0037801101830795213\n",
      "Epoch 555, loss metrics 0.003769855944375971\n",
      "Epoch 556, loss metrics 0.0037596393446533215\n",
      "Epoch 557, loss metrics 0.0037494602457547707\n",
      "Epoch 558, loss metrics 0.003739318510030604\n",
      "Epoch 559, loss metrics 0.003729214000336393\n",
      "Epoch 560, loss metrics 0.0037191465800311264\n",
      "Epoch 561, loss metrics 0.0037091161129752764\n",
      "Epoch 562, loss metrics 0.0036991224635290728\n",
      "Epoch 563, loss metrics 0.0036891654965506395\n",
      "Epoch 564, loss metrics 0.0036792450773941074\n",
      "Epoch 565, loss metrics 0.003669361071907853\n",
      "Epoch 566, loss metrics 0.0036595133464326786\n",
      "Epoch 567, loss metrics 0.003649701767799948\n",
      "Epoch 568, loss metrics 0.003639926203329904\n",
      "Epoch 569, loss metrics 0.0036301865208297457\n",
      "Epoch 570, loss metrics 0.0036204825885919043\n",
      "Epoch 571, loss metrics 0.0036108142753922775\n",
      "Epoch 572, loss metrics 0.003601181450488408\n",
      "Epoch 573, loss metrics 0.0035915839836177226\n",
      "Epoch 574, loss metrics 0.0035820217449958384\n",
      "Epoch 575, loss metrics 0.0035724946053147123\n",
      "Epoch 576, loss metrics 0.0035630024357409616\n",
      "Epoch 577, loss metrics 0.00355354510791408\n",
      "Epoch 578, loss metrics 0.003544122493944716\n",
      "Epoch 579, loss metrics 0.003534734466412978\n",
      "Epoch 580, loss metrics 0.0035253808983666452\n",
      "Epoch 581, loss metrics 0.003516061663319491\n",
      "Epoch 582, loss metrics 0.0035067766352495705\n",
      "Epoch 583, loss metrics 0.0034975256885975184\n",
      "Epoch 584, loss metrics 0.0034883086982648286\n",
      "Epoch 585, loss metrics 0.003479125539612213\n",
      "Epoch 586, loss metrics 0.003469976088457837\n",
      "Epoch 587, loss metrics 0.0034608602210757164\n",
      "Epoch 588, loss metrics 0.003451777814193983\n",
      "Epoch 589, loss metrics 0.003442728744993296\n",
      "Epoch 590, loss metrics 0.0034337128911050693\n",
      "Epoch 591, loss metrics 0.003424730130609957\n",
      "Epoch 592, loss metrics 0.003415780342036061\n",
      "Epoch 593, loss metrics 0.0034068634043573636\n",
      "Epoch 594, loss metrics 0.00339797919699212\n",
      "Epoch 595, loss metrics 0.003389127599801177\n",
      "Epoch 596, loss metrics 0.0033803084930863416\n",
      "Epoch 597, loss metrics 0.0033715217575887688\n",
      "Epoch 598, loss metrics 0.0033627672744874254\n",
      "Epoch 599, loss metrics 0.0033540449253973634\n",
      "Epoch 600, loss metrics 0.0033453545923681992\n",
      "Epoch 601, loss metrics 0.0033366961578824962\n",
      "Epoch 602, loss metrics 0.0033280695048541606\n",
      "Epoch 603, loss metrics 0.003319474516626872\n",
      "Epoch 604, loss metrics 0.0033109110769725444\n",
      "Epoch 605, loss metrics 0.003302379070089636\n",
      "Epoch 606, loss metrics 0.0032938783806017733\n",
      "Epoch 607, loss metrics 0.0032854088935559836\n",
      "Epoch 608, loss metrics 0.0032769704944212805\n",
      "Epoch 609, loss metrics 0.0032685630690870797\n",
      "Epoch 610, loss metrics 0.003260186503861627\n",
      "Epoch 611, loss metrics 0.003251840685470494\n",
      "Epoch 612, loss metrics 0.0032435255010550474\n",
      "Epoch 613, loss metrics 0.0032352408381708564\n",
      "Epoch 614, loss metrics 0.003226986584786326\n",
      "Epoch 615, loss metrics 0.00321876262928096\n",
      "Epoch 616, loss metrics 0.003210568860444075\n",
      "Epoch 617, loss metrics 0.0032024051674731592\n",
      "Epoch 618, loss metrics 0.0031942714399723782\n",
      "Epoch 619, loss metrics 0.003186167567951175\n",
      "Epoch 620, loss metrics 0.0031780934418226595\n",
      "Epoch 621, loss metrics 0.0031700489524022256\n",
      "Epoch 622, loss metrics 0.0031620339909060454\n",
      "Epoch 623, loss metrics 0.0031540484489495694\n",
      "Epoch 624, loss metrics 0.0031460922185460797\n",
      "Epoch 625, loss metrics 0.0031381651921052493\n",
      "Epoch 626, loss metrics 0.003130267262431651\n",
      "Epoch 627, loss metrics 0.003122398322723365\n",
      "Epoch 628, loss metrics 0.0031145582665704236\n",
      "Epoch 629, loss metrics 0.003106746987953532\n",
      "Epoch 630, loss metrics 0.003098964381242476\n",
      "Epoch 631, loss metrics 0.003091210341194826\n",
      "Epoch 632, loss metrics 0.0030834847629543872\n",
      "Epoch 633, loss metrics 0.003075787542049894\n",
      "Epoch 634, loss metrics 0.003068118574393542\n",
      "Epoch 635, loss metrics 0.0030604777562795865\n",
      "Epoch 636, loss metrics 0.0030528649843829534\n",
      "Epoch 637, loss metrics 0.0030452801557577995\n",
      "Epoch 638, loss metrics 0.003037723167836188\n",
      "Epoch 639, loss metrics 0.0030301939184266347\n",
      "Epoch 640, loss metrics 0.0030226923057127737\n",
      "Epoch 641, loss metrics 0.003015218228251986\n",
      "Epoch 642, loss metrics 0.00300777158497397\n",
      "Epoch 643, loss metrics 0.0030003522751794183\n",
      "Epoch 644, loss metrics 0.002992960198538667\n",
      "Epoch 645, loss metrics 0.002985595255090288\n",
      "Epoch 646, loss metrics 0.0029782573452398118\n",
      "Epoch 647, loss metrics 0.002970946369758276\n",
      "Epoch 648, loss metrics 0.0029636622297810333\n",
      "Epoch 649, loss metrics 0.0029564048268062425\n",
      "Epoch 650, loss metrics 0.002949174062693656\n",
      "Epoch 651, loss metrics 0.002941969839663273\n",
      "Epoch 652, loss metrics 0.0029347920602939897\n",
      "Epoch 653, loss metrics 0.0029276406275222638\n",
      "Epoch 654, loss metrics 0.002920515444640896\n",
      "Epoch 655, loss metrics 0.0029134164152975948\n",
      "Epoch 656, loss metrics 0.002906343443493764\n",
      "Epoch 657, loss metrics 0.0028992964335832075\n",
      "Epoch 658, loss metrics 0.002892275290270735\n",
      "Epoch 659, loss metrics 0.0028852799186110155\n",
      "Epoch 660, loss metrics 0.002878310224007178\n",
      "Epoch 661, loss metrics 0.002871366112209602\n",
      "Epoch 662, loss metrics 0.0028644474893146023\n",
      "Epoch 663, loss metrics 0.0028575542617631762\n",
      "Epoch 664, loss metrics 0.00285068633633974\n",
      "Epoch 665, loss metrics 0.0028438436201708725\n",
      "Epoch 666, loss metrics 0.0028370260207240274\n",
      "Epoch 667, loss metrics 0.0028302334458063325\n",
      "Epoch 668, loss metrics 0.0028234658035633103\n",
      "Epoch 669, loss metrics 0.0028167230024776315\n",
      "Epoch 670, loss metrics 0.0028100049513678977\n",
      "Epoch 671, loss metrics 0.0028033115593873994\n",
      "Epoch 672, loss metrics 0.002796642736022893\n",
      "Epoch 673, loss metrics 0.0027899983910933414\n",
      "Epoch 674, loss metrics 0.0027833784347487556\n",
      "Epoch 675, loss metrics 0.0027767827774689423\n",
      "Epoch 676, loss metrics 0.002770211330062297\n",
      "Epoch 677, loss metrics 0.002763664003664572\n",
      "Epoch 678, loss metrics 0.002757140709737747\n",
      "Epoch 679, loss metrics 0.0027506413600687624\n",
      "Epoch 680, loss metrics 0.0027441658667683682\n",
      "Epoch 681, loss metrics 0.002737714142269886\n",
      "Epoch 682, loss metrics 0.0027312860993281046\n",
      "Epoch 683, loss metrics 0.0027248816510180127\n",
      "Epoch 684, loss metrics 0.0027185007107336876\n",
      "Epoch 685, loss metrics 0.0027121431921870737\n",
      "Epoch 686, loss metrics 0.0027058090094068685\n",
      "Epoch 687, loss metrics 0.0026994980767373455\n",
      "Epoch 688, loss metrics 0.002693210308837152\n",
      "Epoch 689, loss metrics 0.0026869456206781747\n",
      "Epoch 690, loss metrics 0.002680703927544469\n",
      "Epoch 691, loss metrics 0.002674485145030998\n",
      "Epoch 692, loss metrics 0.002668289189042521\n",
      "Epoch 693, loss metrics 0.002662115975792513\n",
      "Epoch 694, loss metrics 0.0026559654218020062\n",
      "Epoch 695, loss metrics 0.0026498374438983984\n",
      "Epoch 696, loss metrics 0.0026437319592144113\n",
      "Epoch 697, loss metrics 0.002637648885186949\n",
      "Epoch 698, loss metrics 0.0026315881395559387\n",
      "Epoch 699, loss metrics 0.0026255496403632694\n",
      "Epoch 700, loss metrics 0.0026195333059516436\n",
      "Epoch 701, loss metrics 0.002613539054963526\n",
      "Epoch 702, loss metrics 0.0026075668063400004\n",
      "Epoch 703, loss metrics 0.002601616479319653\n",
      "Epoch 704, loss metrics 0.0025956879934375464\n",
      "Epoch 705, loss metrics 0.0025897812685240864\n",
      "Epoch 706, loss metrics 0.002583896224703927\n",
      "Epoch 707, loss metrics 0.0025780327823949397\n",
      "Epoch 708, loss metrics 0.002572190862307101\n",
      "Epoch 709, loss metrics 0.0025663703854414332\n",
      "Epoch 710, loss metrics 0.0025605712730889054\n",
      "Epoch 711, loss metrics 0.0025547934468293986\n",
      "Epoch 712, loss metrics 0.0025490368285306865\n",
      "Epoch 713, loss metrics 0.002543301340347261\n",
      "Epoch 714, loss metrics 0.0025375869047194214\n",
      "Epoch 715, loss metrics 0.0025318934443721176\n",
      "Epoch 716, loss metrics 0.002526220882313934\n",
      "Epoch 717, loss metrics 0.002520569141836095\n",
      "Epoch 718, loss metrics 0.002514938146511353\n",
      "Epoch 719, loss metrics 0.0025093278201930203\n",
      "Epoch 720, loss metrics 0.0025037380870139097\n",
      "Epoch 721, loss metrics 0.002498168871385277\n",
      "Epoch 722, loss metrics 0.0024926200979958693\n",
      "Epoch 723, loss metrics 0.002487091691810872\n",
      "Epoch 724, loss metrics 0.0024815835780708515\n",
      "Epoch 725, loss metrics 0.002476095682290825\n",
      "Epoch 726, loss metrics 0.0024706279302591917\n",
      "Epoch 727, loss metrics 0.0024651802480367617\n",
      "Epoch 728, loss metrics 0.0024597525619557323\n",
      "Epoch 729, loss metrics 0.00245434479861872\n",
      "Epoch 730, loss metrics 0.002448956884897738\n",
      "Epoch 731, loss metrics 0.0024435887479332123\n",
      "Epoch 732, loss metrics 0.0024382403151330385\n",
      "Epoch 733, loss metrics 0.0024329115141715515\n",
      "Epoch 734, loss metrics 0.0024276022729885403\n",
      "Epoch 735, loss metrics 0.0024223125197883424\n",
      "Epoch 736, loss metrics 0.0024170421830387767\n",
      "Epoch 737, loss metrics 0.002411791191470266\n",
      "Epoch 738, loss metrics 0.002406559474074803\n",
      "Epoch 739, loss metrics 0.0024013469601050514\n",
      "Epoch 740, loss metrics 0.002396153579073341\n",
      "Epoch 741, loss metrics 0.0023909792607507206\n",
      "Epoch 742, loss metrics 0.002385823935166044\n",
      "Epoch 743, loss metrics 0.0023806875326049877\n",
      "Epoch 744, loss metrics 0.002375569983609127\n",
      "Epoch 745, loss metrics 0.0023704712189749804\n",
      "Epoch 746, loss metrics 0.002365391169753081\n",
      "Epoch 747, loss metrics 0.0023603297672470506\n",
      "Epoch 748, loss metrics 0.002355286943012694\n",
      "Epoch 749, loss metrics 0.002350262628856986\n",
      "Epoch 750, loss metrics 0.002345256756837259\n",
      "Epoch 751, loss metrics 0.002340269259260225\n",
      "Epoch 752, loss metrics 0.0023353000686810346\n",
      "Epoch 753, loss metrics 0.002330349117902455\n",
      "Epoch 754, loss metrics 0.0023254163399738822\n",
      "Epoch 755, loss metrics 0.002320501668190425\n",
      "Epoch 756, loss metrics 0.002315605036092113\n",
      "Epoch 757, loss metrics 0.0023107263774628417\n",
      "Epoch 758, loss metrics 0.0023058656263295974\n",
      "Epoch 759, loss metrics 0.0023010227169615502\n",
      "Epoch 760, loss metrics 0.0022961975838690944\n",
      "Epoch 761, loss metrics 0.0022913901618030154\n",
      "Epoch 762, loss metrics 0.0022866003857536467\n",
      "Epoch 763, loss metrics 0.002281828190949879\n",
      "Epoch 764, loss metrics 0.0022770735128584\n",
      "Epoch 765, loss metrics 0.002272336287182745\n",
      "Epoch 766, loss metrics 0.00226761644986247\n",
      "Epoch 767, loss metrics 0.0022629139370722675\n",
      "Epoch 768, loss metrics 0.002258228685221082\n",
      "Epoch 769, loss metrics 0.002253560630951325\n",
      "Epoch 770, loss metrics 0.0022489097111379197\n",
      "Epoch 771, loss metrics 0.0022442758628875023\n",
      "Epoch 772, loss metrics 0.002239659023537584\n",
      "Epoch 773, loss metrics 0.002235059130655664\n",
      "Epoch 774, loss metrics 0.002230476122038426\n",
      "Epoch 775, loss metrics 0.002225909935710864\n",
      "Epoch 776, loss metrics 0.0022213605099254416\n",
      "Epoch 777, loss metrics 0.002216827783161284\n",
      "Epoch 778, loss metrics 0.0022123116941233476\n",
      "Epoch 779, loss metrics 0.002207812181741555\n",
      "Epoch 780, loss metrics 0.0022033291851699956\n",
      "Epoch 781, loss metrics 0.0021988626437861257\n",
      "Epoch 782, loss metrics 0.0021944124971898737\n",
      "Epoch 783, loss metrics 0.002189978685202909\n",
      "Epoch 784, loss metrics 0.0021855611478677696\n",
      "Epoch 785, loss metrics 0.002181159825447089\n",
      "Epoch 786, loss metrics 0.0021767746584227584\n",
      "Epoch 787, loss metrics 0.0021724055874951308\n",
      "Epoch 788, loss metrics 0.0021680525535822484\n",
      "Epoch 789, loss metrics 0.002163715497818977\n",
      "Epoch 790, loss metrics 0.0021593943615562912\n",
      "Epoch 791, loss metrics 0.002155089086360439\n",
      "Epoch 792, loss metrics 0.0021507996140121112\n",
      "Epoch 793, loss metrics 0.002146525886505746\n",
      "Epoch 794, loss metrics 0.0021422678460486706\n",
      "Epoch 795, loss metrics 0.0021380254350603484\n",
      "Epoch 796, loss metrics 0.0021337985961716044\n",
      "Epoch 797, loss metrics 0.002129587272223823\n",
      "Epoch 798, loss metrics 0.0021253914062682225\n",
      "Epoch 799, loss metrics 0.0021212109415650318\n",
      "Epoch 800, loss metrics 0.002117045821582752\n",
      "Epoch 801, loss metrics 0.002112895989997402\n",
      "Epoch 802, loss metrics 0.0021087613906917253\n",
      "Epoch 803, loss metrics 0.002104641967754449\n",
      "Epoch 804, loss metrics 0.002100537665479538\n",
      "Epoch 805, loss metrics 0.0020964484283654285\n",
      "Epoch 806, loss metrics 0.002092374201114271\n",
      "Epoch 807, loss metrics 0.0020883149286311893\n",
      "Epoch 808, loss metrics 0.002084270556023561\n",
      "Epoch 809, loss metrics 0.00208024102860021\n",
      "Epoch 810, loss metrics 0.0020762262918707594\n",
      "Epoch 811, loss metrics 0.0020722262915448026\n",
      "Epoch 812, loss metrics 0.002068240973531213\n",
      "Epoch 813, loss metrics 0.0020642702839374434\n",
      "Epoch 814, loss metrics 0.0020603141690687375\n",
      "Epoch 815, loss metrics 0.0020563725754274213\n",
      "Epoch 816, loss metrics 0.002052445449712192\n",
      "Epoch 817, loss metrics 0.0020485327388174226\n",
      "Epoch 818, loss metrics 0.0020446343898323947\n",
      "Epoch 819, loss metrics 0.0020407503500405764\n",
      "Epoch 820, loss metrics 0.0020368805669189836\n",
      "Epoch 821, loss metrics 0.002033024988137384\n",
      "Epoch 822, loss metrics 0.002029183561557657\n",
      "Epoch 823, loss metrics 0.002025356235233035\n",
      "Epoch 824, loss metrics 0.002021542957407438\n",
      "Epoch 825, loss metrics 0.002017743676514748\n",
      "Epoch 826, loss metrics 0.0020139583411781558\n",
      "Epoch 827, loss metrics 0.0020101869002093946\n",
      "Epoch 828, loss metrics 0.0020064293026081175\n",
      "Epoch 829, loss metrics 0.002002685497561144\n",
      "Epoch 830, loss metrics 0.0019989554344418616\n",
      "Epoch 831, loss metrics 0.0019952390628094367\n",
      "Epoch 832, loss metrics 0.0019915363324082115\n",
      "Epoch 833, loss metrics 0.001987847193166971\n",
      "Epoch 834, loss metrics 0.001984171595198323\n",
      "Epoch 835, loss metrics 0.0019805094887979627\n",
      "Epoch 836, loss metrics 0.001976860824444043\n",
      "Epoch 837, loss metrics 0.0019732255527964812\n",
      "Epoch 838, loss metrics 0.0019696036246963197\n",
      "Epoch 839, loss metrics 0.001965994991165004\n",
      "Epoch 840, loss metrics 0.001962399603403811\n",
      "Epoch 841, loss metrics 0.0019588174127930986\n",
      "Epoch 842, loss metrics 0.0019552483708917\n",
      "Epoch 843, loss metrics 0.0019516924294362672\n",
      "Epoch 844, loss metrics 0.0019481495403405726\n",
      "Epoch 845, loss metrics 0.0019446196556949346\n",
      "Epoch 846, loss metrics 0.0019411027277654906\n",
      "Epoch 847, loss metrics 0.0019375987089936125\n",
      "Epoch 848, loss metrics 0.001934107551995248\n",
      "Epoch 849, loss metrics 0.0019306292095602243\n",
      "Epoch 850, loss metrics 0.0019271636346517263\n",
      "Epoch 851, loss metrics 0.0019237107804055242\n",
      "Epoch 852, loss metrics 0.0019202706001294509\n",
      "Epoch 853, loss metrics 0.0019168430473027224\n",
      "Epoch 854, loss metrics 0.0019134280755752845\n",
      "Epoch 855, loss metrics 0.001910025638767251\n",
      "Epoch 856, loss metrics 0.0019066356908682125\n",
      "Epoch 857, loss metrics 0.0019032581860366565\n",
      "Epoch 858, loss metrics 0.0018998930785993405\n",
      "Epoch 859, loss metrics 0.001896540323050657\n",
      "Epoch 860, loss metrics 0.0018931998740520587\n",
      "Epoch 861, loss metrics 0.0018898716864313788\n",
      "Epoch 862, loss metrics 0.0018865557151822754\n",
      "Epoch 863, loss metrics 0.0018832519154636101\n",
      "Epoch 864, loss metrics 0.0018799602425988405\n",
      "Epoch 865, loss metrics 0.0018766806520754012\n",
      "Epoch 866, loss metrics 0.0018734130995440976\n",
      "Epoch 867, loss metrics 0.0018701575408185808\n",
      "Epoch 868, loss metrics 0.0018669139318746354\n",
      "Epoch 869, loss metrics 0.0018636822288496482\n",
      "Epoch 870, loss metrics 0.0018604623880420243\n",
      "Epoch 871, loss metrics 0.0018572543659105825\n",
      "Epoch 872, loss metrics 0.0018540581190739393\n",
      "Epoch 873, loss metrics 0.0018508736043099687\n",
      "Epoch 874, loss metrics 0.001847700778555186\n",
      "Epoch 875, loss metrics 0.0018445395989041624\n",
      "Epoch 876, loss metrics 0.0018413900226089699\n",
      "Epoch 877, loss metrics 0.0018382520070786001\n",
      "Epoch 878, loss metrics 0.0018351255098783452\n",
      "Epoch 879, loss metrics 0.0018320104887293057\n",
      "Epoch 880, loss metrics 0.001828906901507725\n",
      "Epoch 881, loss metrics 0.0018258147062444857\n",
      "Epoch 882, loss metrics 0.0018227338611245232\n",
      "Epoch 883, loss metrics 0.001819664324486253\n",
      "Epoch 884, loss metrics 0.0018166060548209996\n",
      "Epoch 885, loss metrics 0.001813559010772473\n",
      "Epoch 886, loss metrics 0.0018105231511361896\n",
      "Epoch 887, loss metrics 0.0018074984348588632\n",
      "Epoch 888, loss metrics 0.001804484821037952\n",
      "Epoch 889, loss metrics 0.0018014822689210095\n",
      "Epoch 890, loss metrics 0.0017984907379052012\n",
      "Epoch 891, loss metrics 0.001795510187536716\n",
      "Epoch 892, loss metrics 0.0017925405775102334\n",
      "Epoch 893, loss metrics 0.0017895818676683737\n",
      "Epoch 894, loss metrics 0.001786634018001164\n",
      "Epoch 895, loss metrics 0.0017836969886454873\n",
      "Epoch 896, loss metrics 0.0017807707398845504\n",
      "Epoch 897, loss metrics 0.0017778552321473346\n",
      "Epoch 898, loss metrics 0.001774950426008081\n",
      "Epoch 899, loss metrics 0.0017720562821857467\n",
      "Epoch 900, loss metrics 0.0017691727615434729\n",
      "Epoch 901, loss metrics 0.0017662998250880296\n",
      "Epoch 902, loss metrics 0.0017634374339693566\n",
      "Epoch 903, loss metrics 0.0017605855494799642\n",
      "Epoch 904, loss metrics 0.0017577441330544608\n",
      "Epoch 905, loss metrics 0.00175491314626898\n",
      "Epoch 906, loss metrics 0.0017520925508407452\n",
      "Epoch 907, loss metrics 0.0017492823086274715\n",
      "Epoch 908, loss metrics 0.001746482381626846\n",
      "Epoch 909, loss metrics 0.0017436927319761076\n",
      "Epoch 910, loss metrics 0.001740913321951433\n",
      "Epoch 911, loss metrics 0.001738144113967486\n",
      "Epoch 912, loss metrics 0.0017353850705768702\n",
      "Epoch 913, loss metrics 0.0017326361544696713\n",
      "Epoch 914, loss metrics 0.0017298973284728893\n",
      "Epoch 915, loss metrics 0.0017271685555499928\n",
      "Epoch 916, loss metrics 0.001724449798800403\n",
      "Epoch 917, loss metrics 0.0017217410214589721\n",
      "Epoch 918, loss metrics 0.0017190421868955065\n",
      "Epoch 919, loss metrics 0.0017163532586142527\n",
      "Epoch 920, loss metrics 0.00171367420025345\n",
      "Epoch 921, loss metrics 0.0017110049755847726\n",
      "Epoch 922, loss metrics 0.0017083455485129018\n",
      "Epoch 923, loss metrics 0.0017056958830749906\n",
      "Epoch 924, loss metrics 0.0017030559434402005\n",
      "Epoch 925, loss metrics 0.0017004256939092243\n",
      "Epoch 926, loss metrics 0.001697805098913765\n",
      "Epoch 927, loss metrics 0.0016951941230161128\n",
      "Epoch 928, loss metrics 0.0016925927309086087\n",
      "Epoch 929, loss metrics 0.001690000887413227\n",
      "Epoch 930, loss metrics 0.0016874185574810192\n",
      "Epoch 931, loss metrics 0.001684845706191716\n",
      "Epoch 932, loss metrics 0.0016822822987532368\n",
      "Epoch 933, loss metrics 0.0016797283005011718\n",
      "Epoch 934, loss metrics 0.0016771836768983898\n",
      "Epoch 935, loss metrics 0.0016746483935345018\n",
      "Epoch 936, loss metrics 0.0016721224161254397\n",
      "Epoch 937, loss metrics 0.0016696057105129564\n",
      "Epoch 938, loss metrics 0.0016670982426642159\n",
      "Epoch 939, loss metrics 0.0016645999786712926\n",
      "Epoch 940, loss metrics 0.0016621108847506983\n",
      "Epoch 941, loss metrics 0.0016596309272429925\n",
      "Epoch 942, loss metrics 0.0016571600726122365\n",
      "Epoch 943, loss metrics 0.0016546982874456294\n",
      "Epoch 944, loss metrics 0.001652245538452998\n",
      "Epoch 945, loss metrics 0.0016498017924663501\n",
      "Epoch 946, loss metrics 0.0016473670164394623\n",
      "Epoch 947, loss metrics 0.001644941177447402\n",
      "Epoch 948, loss metrics 0.0016425242426860716\n",
      "Epoch 949, loss metrics 0.0016401161794718229\n",
      "Epoch 950, loss metrics 0.0016377169552409298\n",
      "Epoch 951, loss metrics 0.0016353265375492261\n",
      "Epoch 952, loss metrics 0.0016329448940716176\n",
      "Epoch 953, loss metrics 0.0016305719926016898\n",
      "Epoch 954, loss metrics 0.0016282078010512032\n",
      "Epoch 955, loss metrics 0.0016258522874497265\n",
      "Epoch 956, loss metrics 0.0016235054199441766\n",
      "Epoch 957, loss metrics 0.0016211671667983772\n",
      "Epoch 958, loss metrics 0.001618837496392656\n",
      "Epoch 959, loss metrics 0.0016165163772233902\n",
      "Epoch 960, loss metrics 0.0016142037779026165\n",
      "Epoch 961, loss metrics 0.0016118996671575577\n",
      "Epoch 962, loss metrics 0.0016096040138302427\n",
      "Epoch 963, loss metrics 0.0016073167868770502\n",
      "Epoch 964, loss metrics 0.0016050379553683427\n",
      "Epoch 965, loss metrics 0.0016027674884879454\n",
      "Epoch 966, loss metrics 0.0016005053555328856\n",
      "Epoch 967, loss metrics 0.0015982515259128165\n",
      "Epoch 968, loss metrics 0.0015960059691497014\n",
      "Epoch 969, loss metrics 0.0015937686548773934\n",
      "Epoch 970, loss metrics 0.0015915395528411577\n",
      "Epoch 971, loss metrics 0.0015893186328973664\n",
      "Epoch 972, loss metrics 0.0015871058650130168\n",
      "Epoch 973, loss metrics 0.001584901219265319\n",
      "Epoch 974, loss metrics 0.0015827046658413524\n",
      "Epoch 975, loss metrics 0.0015805161750376143\n",
      "Epoch 976, loss metrics 0.0015783357172596292\n",
      "Epoch 977, loss metrics 0.0015761632630215407\n",
      "Epoch 978, loss metrics 0.0015739987829457504\n",
      "Epoch 979, loss metrics 0.0015718422477624472\n",
      "Epoch 980, loss metrics 0.001569693628309319\n",
      "Epoch 981, loss metrics 0.001567552895531046\n",
      "Epoch 982, loss metrics 0.0015654200204789684\n",
      "Epoch 983, loss metrics 0.0015632949743107061\n",
      "Epoch 984, loss metrics 0.0015611777282897176\n",
      "Epoch 985, loss metrics 0.0015590682537849714\n",
      "Epoch 986, loss metrics 0.0015569665222704891\n",
      "Epoch 987, loss metrics 0.0015548725053250452\n",
      "Epoch 988, loss metrics 0.0015527861746317108\n",
      "Epoch 989, loss metrics 0.0015507075019774855\n",
      "Epoch 990, loss metrics 0.0015486364592529498\n",
      "Epoch 991, loss metrics 0.001546573018451849\n",
      "Epoch 992, loss metrics 0.0015445171516707463\n",
      "Epoch 993, loss metrics 0.0015424688311085962\n",
      "Epoch 994, loss metrics 0.0015404280290664183\n",
      "Epoch 995, loss metrics 0.0015383947179469096\n",
      "Epoch 996, loss metrics 0.0015363688702540398\n",
      "Epoch 997, loss metrics 0.001534350458592723\n",
      "Epoch 998, loss metrics 0.0015323394556684465\n",
      "Epoch 999, loss metrics 0.001530335834286841\n",
      "Epoch 1000, loss metrics 0.0015283395673534\n"
     ]
    }
   ],
   "source": [
    "lm.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "06a0f7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2042fe778b0>]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNBElEQVR4nO3dd3gU5cLG4d9uyiakkQBphN5C7yBFAUEQEUWwcVBAQEFBQaxwbNiwFywggmBDUJpIFUVABGkSeu8lCTWdbJLd+f5YD5oPogSSnST73NeV65x39p3k2Zyw+5yZd2cshmEYiIiIiJjEanYAERER8WwqIyIiImIqlRERERExlcqIiIiImEplREREREylMiIiIiKmUhkRERERU6mMiIiIiKm8zQ5wOZxOJydOnCAoKAiLxWJ2HBEREbkMhmGQmppKdHQ0Vmvexz+KRRk5ceIEFSpUMDuGiIiIXIGjR48SExOT5+PFoowEBQUBricTHBxschoRERG5HCkpKVSoUOHC+3heikUZ+d+pmeDgYJURERGRYubfllhoAauIiIiYSmVERERETKUyIiIiIqZSGRERERFTqYyIiIiIqVRGRERExFQqIyIiImIqlRERERExlcqIiIiImEplREREREylMiIiIiKmUhkRERERU6mMiIiIeLBFW+MZ8uVGHE7DtAzF4q69IiIiUrAysx28unAnX6w5DMB3G45yd4uKpmTJ15GR8ePH06BBA4KDgwkODqZVq1YsWrToH/f57rvviI2Nxc/Pj/r167Nw4cKrCiwiIiJX5+DpdHqNX32hiAxpV41eTWNMy5OvMhITE8Nrr73Gxo0b2bBhA9dffz233nor27dvv+T81atX07t3bwYOHMimTZvo0aMHPXr0YNu2bQUSXkRERPLn+7jj3DzuV7afSCEswJcp9zXn6a6x+HiZt3LDYhjGVZ0kCgsL480332TgwIEXPXbXXXeRnp7O/PnzL2y75ppraNSoERMmTLjsn5GSkkJISAjJyckEBwdfTVwRERGPlJntYMwP2/lm3VEAWlQOY1zvxkSG+BXaz7zc9+8rrkEOh4Pp06eTnp5Oq1atLjlnzZo1dOrUKde2Ll26sGbNmn/83na7nZSUlFxfIiIicmX2nUzl1g9/45t1R7FY4OHrqzPt/pauInJ8I/z4LFzdsYmrku8FrFu3bqVVq1ZkZmYSGBjInDlzqFOnziXnJiQkEBERkWtbREQECQkJ//gzxo4dy5gxY/IbTURERP6fmRuP8ezcbZzPdlA20MZ7dzWibY2y4HTCqndh2cvgzIHIBtDgDlMy5ruM1KpVi7i4OJKTk5k5cyb9+vVjxYoVeRaSKzFq1ChGjhx5YZySkkKFChUK7PuLiIiUdBlZOTw7dzuz/jgGQOtqZXjv7kaEB/lBygmYMxgOrnRNrtMDanTK+5sVsnyXEV9fX6pXrw5A06ZNWb9+Pe+//z6ffPLJRXMjIyNJTEzMtS0xMZHIyMh//Bk2mw2bzZbfaCIiIgLsSkhh6Nd/sP9UOlYLjOhUk6EdquNltcDOH2Dew3D+HPgEwE1vQKM+YLGYlveql846nU7sdvslH2vVqhU///xzrm1Lly7Nc42JiIiIXDnDMJi+7gi3fvgb+0+lExFsY9r91/BIxxp45WTAD8Nhxj2uIhLVCAavhMb3mFpEIJ9HRkaNGkXXrl2pWLEiqampTJs2jeXLl7NkyRIA+vbtS/ny5Rk7diwAw4cPp127drz99tt069aN6dOns2HDBiZOnFjwz0RERMSDpdlzGD17K/M2nwCgXc1yvHNnQ8oE2iB+C8waCKf3uCa3GQ4dngFvXxMT/yVfZeTkyZP07duX+Ph4QkJCaNCgAUuWLOGGG24A4MiRI1itfx1sad26NdOmTeOZZ55h9OjR1KhRg7lz51KvXr2CfRYiIiIebNvxZIZN+4NDZzLwslp4vHMtBl9XFSsGrPkIfnoBHFkQGAm3TYBqHcyOnMtVX2fEHXSdEREREddpmFNpdnbFp7I7IZWdCSnsik9lT2IqOU6D6BA/PvhPY5pWCoPURJg7BPYvc+1cqxvc8gEElHFb3st9/9a9aURERIqgzGwH+06msTM+hV0Jqez6s3icSc+65PxOtSN48/YGhAb4wp4lMPchyDgN3n7Q5VVoNsD0tSF5URkRERExWWpmNusOnmVXQuqF8nHwdPol76RrsUCVsgHERgYRGxlMbGQQtaOCqRBWCrIzYeGTsO7PT7hG1INekyE81s3PKH9URkREREyUkJxJz49/40Ry5kWPlS7lQ+3IYGKjgi78Z43wIPx9vS7+Rok7XItUT+5wja95CDo+Dz6Fd7n3gqIyIiIiYpLzWQ4e+HIDJ5IzCQ+y0bpaGWKj/jraER5kw/Jvp1YMA9ZPgiX/BYcdAspBj/FQ4wb3PIkCoDIiIiJiAsMweGLmZrYcSya0lA8zh7SmYplS+fsm6afh+6GwZ7FrXP0G6PExBIYXfOBCpDIiIiJigg+X7WP+lni8rRbG39M0/0Vk388w90FISwQvX7jhRWg5pMguUv0nKiMiIiJutnhbPG8vdV2A7KUe9bimaj4+bptjh59fhDUfusblYqHXJIisXwhJ3UNlRERExI22n0jm0RmbAejfujK9W1S8/J1P7XEtUk3Y4ho3GwidXwbffB5VKWJURkRERNzkZGom93++gfPZDq6tUZZnutW+vB0NA/74AhY/DdkZ4B8Gt34Isd0KN7CbqIyIiIi4QWa2g8FfbuREciZVywbw4X+a4O11GferzTgLPzziutsuQJV2cNsnEBxVuIHdSGVERESkkBmGweg5W9l0JIlgP28m9WtGiL/Pv+948FeY/QCkngCrN3R8Dlo9DNbLKDHFiMqIiIhIIZu48gCz/ziOl9XCx32aUrVc4D/v4MiG5WPh13cAA8Kqwe2TIbqxW/K6m8qIiIhIIfppRyKvLd4FwPPd69C2Rtl/3uHsAZg1CI5vdI0b3ws3vga2fykwxZjKiIiISCHZnZDK8OmbMAzo07Ii915TKe/JhgGbp8PCxyErDfxCoPv7UPc29wU2icqIiIhIITiTZmfg5+tJz3LQqmoZXrilbt6Xds9MhvkjYdtM17hSG9ci1dIV3BfYRCojIiIiBSwrx8mDX//BsXPnqVSmFB/3aYJPXp+cObLWdVom+QhYvKDDKGg7EqyXuBleCaUyIiIiUoAMw+C577ex7uBZgmzeTO7XjNAA34snOnLg17dgxetgOKF0Jeg1GSo0d39ok6mMiIiIFKApvx1i+vqjWC0w7j+NqR4edPGkc4ddH9k9+rtr3OAuuOkt8At2b9giQmVERESkgKzYc4qXF+wAYPRNtelQ6xJ3z906E+Y/CvYU8A2Cm9+BBne6OWnRojIiIiJSAPadTGPYtD9wGnBnsxgGtq2Se4I9FRY+CZunucYxzV03uAut7PasRY3KiIiIyFU6m57FoM/Xk5qZQ/PKobzUo17uT84c2+i6wd25g2CxwrWPQ7unwEtvw6AyIiIiclVW7z/NyBmbSUjJpHxpf8bf0xSb95+fhHE64Lf34JdXwZkDIRWg50So1NrUzEWNyoiIiMgVyMpx8s7SPXyycj+GAVXLBjCxb1PKBtpcE5KPw5zBcOhX17jubXDze+Bf2qzIRZbKiIiISD4dOJXG8OlxbD2eDMDdzSvwXPc6lPL98211xzyY9zBkJoFPANz0BjTqA3ld9MzDqYyIiIhcJsMwmLH+KGN+2MH5bAelS/nwWs/63FgvyjUhKx0Wj4I/PneNoxu7rh1Sppp5oYsBlREREZHLkJSRxajZW1m0LQGA1tXK8M6djYgM8XNNiN8MMwfCmb2ABdqOgPajwfsSFzyTXFRGRERE/sXqfacZ+a1rkaqPl4XHO9fi/murYrVawOmE3z+Cn8aAMxuColz3lanazuzYxYbKiIiISB6ycpy8vXQ3E1ceuLBI9f27G1M/JsQ1ITUB5gyBA7+4xrE3wy0fQKkw80IXQyojIiIil7D/VBoj/rZItXeLijx7c+2/FqnuXgzfPwQZZ8DbH24cC037a5HqFVAZERER+RvDMJi+/igv5lqk2oAb60W6JmSfh6XPwbqJrnFEfbh9MpSrZV7oYk5lRERE5E/n0rN4evYWlmxPBKBN9TK8fcffFqkm7nBdSfWk6/4zXDMUOj0P3jaTEpcMKiMiIiLkvpKqj5eFJ7rUYlDbPxepGgas+xR+fAYcdggIhx7joUYns2OXCCojIiLi8VbsOcXAqevJcRpULRfAuLsbU6/8n4tU00/D3Idg7xLXuEZnuPVjCCxnXuASRmVEREQ8WtzRJB78aiM5ToOb6kfy1h0N/1qkuu9nmPsgpCWClw06vwQtHtAi1QKmMiIiIh5r/6k07puyjowsB9fWKMt7dzXG19sKOXb4+UVY86FrYrlY15VUI+uZG7iEUhkRERGPlJCcSd/J6ziXkU3DmBAm3NPUVURO7YFZAyBhq2ti8/tdR0R8/M0NXIKpjIiIiMdJPp9Nv8/WcTzpPFXKBvBZ/+YE+HrBhimue8vknAf/MLj1I4i9yey4JZ7KiIiIeJTMbAf3f76B3YmphAfZ+GJAC8pY02HGANg13zWpanvoMQGCo0zN6ilURkRExGPkOJw88s0m1h06S5DNm88HtKBC8gaYMhhST4DVBzo+B62GgdVqdlyPoTIiIiIewTAMnv1+Gz/uSMTX28qkextSe/s7sOo9wIAy1aHXJIhubHZUj6MyIiIiHuHdpXv4Zt1RrBaYdHMoLZf1hhN/uB5s0hdufA18A8wN6aFURkREpMT7fPUhxi3bBxh81ewArZc9AFlp4BcC3cdB3R5mR/RoKiMiIlKizd9yghd+2E4w6XwX8x21tv7oeqBSG+g5EUJizA0oKiMiIlJy/bbvNI/OiKMpu/g0aCKhpxPA4gUdRkPbR8HqZXZEQWVERERKqG3Hk3noy3UMtXzHI7a5WLOdEFrZdSXVmGZmx5O/URkREZES5/CZdEZ9Np/Jxns0897j2tjgbrjpTfALNjecXERlRERESpRTqXamfvIWX+eMJ9h6HsMWhKXbu9DgDrOjSR5URkREpMRITT7L5o8G8nzWMrBAdlQzfO6c7Do9I0WWyoiIiJQIWYfXkf5FXzo54nFgJaX5cEJvfAa89FZX1Ol/IRERKd6cDk4vfp3S694iEgcnjLKc7z6Bas1uMDuZXCaVERERKbayzx4h8fN+xCS7rqS6mFaUvvMjrqlbzeRkkh8qIyIiUiwdWfUNoT8/ToyRRrphY0bZR7jp3seILO1vdjTJJ5UREREpVuwZKez+/GEaJM4FYDvVONHpA+5r2xqLxWJuOLkiKiMiIlJs7Nn0K/4/DKaB8zhOw8KPoXfTpP8b1C2ta4cUZyojIiJS5GVmZbP6qxdpe/gjfC0OEgnj4HXvcGPH28yOJgVAZURERIq0zTt3kT1zMNc74sACmwOvpWL/SVxTNtLsaFJAVEZERKRIOp/l4PsZk7hh30uUsaSSiS/7mz5Dw5sfAa0NKVFURkREpMhZu/s4x797nLtzFoIFjvvVILjP59StUNfsaFIIVEZERKTISLPnMHX2Am7YOZqW1mMAHKk1gIp3vAbeNpPTSWFRGRERkSJh46EzrPz6VR7K+hybNZtU7zCsvT6hYu3OZkeTQqYyIiIipnI4Dab+uI6qq5/kUWscWOBs+Q6E9f4UAsuZHU/cQGVERERMk5iSyWdTJzHozBuUsyaTbfHB0fFFwto8qEWqHkRlRERETPHL9qMcn/k0o4z5YIHkoOoE9/kcn8h6ZkcTN1MZERERt7LnOPhszhLabX2aDtbDACTXv4+QW8aCj+4r44lURkRExG0OnkpjwdRXGZj2Kf7WLNK9S+PTczwhdW4yO5qYSGVERETcYv7v2/BbNIJhlvVggTMRbShzz2cQpCupejqVERERKVRp9hw+//oLeh1+kUjLOXLwJuO6ZyjTfjhYrWbHkyJAZURERArNtsOn2PLVkzyYNQerxeCsfyVC7vmc4PKNzY4mRUi+KunYsWNp3rw5QUFBhIeH06NHD3bv3v2P+0ydOhWLxZLry8/P76pCi4hI0WYYBt8u+QXn5M78J3s2VovByZp3E/boGrxUROT/ydeRkRUrVjB06FCaN29OTk4Oo0ePpnPnzuzYsYOAgIA89wsODs5VWiz67LiISIl1JjWTOVPfpPfpDwiw2km3BkH39wlv3MvsaFJE5auMLF68ONd46tSphIeHs3HjRq677ro897NYLERGaoGSiEhJleNwsv1ECpv3Hiby11EMMn4DCySENiOi31QspSuYHVGKsKtaM5KcnAxAWFjYP85LS0ujUqVKOJ1OmjRpwquvvkrdunnfedFut2O32y+MU1JSriamiIgUsHR7DnFHk1h38CwbDp9l05Ek6mRv5z3fj4mxnMaBlTPNHyey69Ng9TI7rhRxFsMwjCvZ0el0csstt5CUlMSqVavynLdmzRr27t1LgwYNSE5O5q233mLlypVs376dmJiYS+7zwgsvMGbMmIu2JycnExwcfCVxRUTkKpxKtbPx8FnWHzrH+kNn2X4iBYfT9fbhhYNHvOcwzHsOXhik+MXge9dk/KpcY3JqMVtKSgohISH/+v59xWXkwQcfZNGiRaxatSrPUnEp2dnZ1K5dm969e/PSSy9dcs6ljoxUqFBBZURExA0Mw+DwmQzWHzrL+kNn2XDoHAdOp180r3xpfzpHZ/LQ2dcol7TZtbFhb+j6BvjptVouv4xc0WmaYcOGMX/+fFauXJmvIgLg4+ND48aN2bdvX55zbDYbNpvtSqKJiMhVSEjOpN9n69idmJpru8UCtSKCaFY5lOaVw2hWOYzyR+bDgpFgTwFbMNz8LtS/3aTkUpzlq4wYhsHDDz/MnDlzWL58OVWqVMn3D3Q4HGzdupWbbtKlf0VEiprXFu1kd2Iqvl5WGlYIoVnlMJpXDqVpxTBCSvm4JmWmwMIRsGW6a1yhJfT8FEIrmZZbird8lZGhQ4cybdo0vv/+e4KCgkhISAAgJCQEf3/XzY369u1L+fLlGTt2LAAvvvgi11xzDdWrVycpKYk333yTw4cPM2jQoAJ+KiIicjXijiYxN+4EFgvMerA19WNCLp50bAPMGgjnDoHFCtc9Cdc9AV66hqZcuXz99YwfPx6A9u3b59o+ZcoU+vfvD8CRI0ew/u3yvufOneP+++8nISGB0NBQmjZtyurVq6lTp87VJRcRkQJjGAYvzd8BQM/GMRcXEacDVr0Dv4wFwwEhFaHnRKjUyoS0UtJc8QJWd7rcBTAiInJlfth8goe/2YS/jxfLn2hPRPDfrpSdfAxmPwCHf3ON6/WCbu+Af2lTskrxUagLWEVEpOTIzHbw2qJdAAxpVy13Edk+F34YDplJ4BsIN70FDe92rWgVKSAqIyIiHm7yqoMcTzpPVIgfD1xX1bUxKx0WPQWbvnSNo5tAr0lQppp5QaXEUhkREfFgJ1Mz+fgX16UWnroxFn9fLzgR51qkemYfYIG2j0KH0eDlY2pWKblURkREPNg7P+4hPctBwwqluaVBJPw2Dn5+EZzZEBTtWqRa5VqzY0oJpzIiIuKhdpxIYcaGowC8eH0ZrF/3hAPLXQ/G3gy3fACl/vneYyIFQWVERMQDGYbBywt2YBjwdNWDNPzhYcg4Az6l4Max0KSfFqmK26iMiIh4oJ92nmTj/nhe8Z1GnxM/ujZGNoBek6FcTXPDicdRGRER8TBZOU6m/7CIH3zfoKb1uGtjq2HQ8Tnw1n3BxP1URkREPIlhsPHbsXyc8S42azbOgAist42H6h3NTiYeTGVERMRTpJ0ie/YQWh34CSxwPLwd5ft9BgFlzU4mHs7671NERKTY2/sTjG+Nz4GfsBs+fOQ/hMjBc1VEpEhQGRERKcly7LB4FHzdC9JPsttZge5ZL9O41xN4eektQIoGnaYRESmpTu2GmQMhcSsAPwX1YOipHlxbuwKtq+uIiBQdKiMiIiWNYcDGKbB4NOSch1Jl2N58LIOWBOJttTD6plizE4rkomN0IiIlSfoZmHEPzH/UVUSqXU/O4N8YGRcFQN9WlalaLtDkkCK56ciIiEhJcWA5zBkCqfFg9YEbxkDLB5mx/ii7E1MpXcqH4R1rmJ1S5CIqIyIixV1OFvzyCvz2PmBA2ZrQaxJENSQlM5t3ftwDwIiONQgppTvvStGjMiIiUpyd3gezBkJ8nGvctD90eRV8AwD46Jd9nEnPolq5APpcU8m0mCL/RGVERKQ4MgzY9BUsegqy08E/1HWX3drdL0w5ciaDKasOAfDfbrXx0Ud5pYhSGRERKW7On3MtUN0+xzWufC3c9gmElM817bXFO8lyOLm2Rlk61Ao3IajI5VEZEREpTg6vhln3Q8oxsHpDh/9Cm+Fg9co1bd3BsyzcmoDVAs90q4PFYjEpsMi/UxkRESkOHNmw4nX49W0wnBBW1bVItXzTi6Y6nQYvzd8BwN0tKlIrMsjdaUXyRWVERKSoO3sQZt8Px9a7xo36QNfXwXZxyUjJzObLNYfZejyZIJs3I2+o6eawIvmnMiIiUpRt+Rbmj4SsVLCFwM3vQP3bLzxsGAb7T6Xzy66TLNt1kvWHzpLjNAAYen11ygbazEouctlURkREiqLMFFj4OGyZ4RpXuAZ6ToTQSmRmO/j9wBl+2XWSX3af4sjZjFy7Vi0bQPeG0QxsW8WE4CL5pzIiIlLUHF3vunZI0mGwWKHdU5xoMJRf9pzll13r+W3fGc5nOy5M9/Wy0rJqGB1qhXN9bDiVywaYGF4k/1RGRESKCqcDfn0Hlo8Fw4E9oDwzKz/Pl3FR7Fq8MtfUyGA/OsSWo0OtcNpUL0uATS/nUnzpr1dEpChIOgqzH4AjqwFYYmnDE2f6k3ImAEjFaoHGFUO5Pjac9rXKUScqWB/XlRJDZURExGzb58APwyEzmUyrP6Mz+zHbeS0h/r7cUrMc18eG065mOUIDfM1OKlIoVEZERMxiT4PFT7ku6w5st9TgwfMPcdwSyYhO1Rnaobou4S4eQWVERMQMJzbBzIFwdj8GFj7OuYV3c3oRHRbMt3c1ommlULMTiriNyoiIiDs5nbB6HCx7GZzZnLKW5eHMIfzurMOdzWJ4rntdArUYVTyM/uJFRNwlJR7mDIaDKwD40WjBExmDsJQKZULP+txYL8rkgCLmUBkREXGHXQvg+2Fw/ix2ix/PZd3LDEd72lYvx1t3NCQyxM/shCKmURkRESlMWRnw4zOwYTIAu6jCQ5lDOeYVw7M3x3Jf68pYrfqIrng2lRERkcKSsM11JdVTuwD4JKcbb+fcSdXIMObd3YjYyGCTA4oUDSojIiIFzTBg7Sew9FlwZHHGEspw+xBWOeszsG0VnuhSCz8fL7NTihQZKiMiIgUp7STMfQj2LQXgZ2cTnsh6AJ/gcnx1RyPa1ihrckCRokdlRESkoOz9CeYOgfRT2PHl5ez/8KXjBrrWi+LV2+rrCqoieVAZERG5WtmZ8PMY+P1jAHY7K/Bw9jCO+1TmzdvqcnvTGN1HRuQfqIyIiFyNk7swZg3AkrgdgCk5XXgtpzc3NKjE591qExXib3JAkaJPZURE5EoYBmz4DOfiUVgddk4bwTyRPZhjZa9lyq11aV1Na0NELpfKiIhIfqWfIWvOUHz3LcIKrHA04DnrMO7t2px+rSvr5nYi+aQyIiKSD459y7F/N4hS9lPYDW9ez+lNUv0BfHdTHcKDdRVVkSuhMiIicjlyskj4/hnCt06kFAb7nNG8E/wU/XvdQosqYWanEynWVEZERP7F2SM7SJ/WnwqZuwH41uhEZqeXGNcmFm+dkhG5aiojIiJ5yMlxsHbOOJpsH0sYds4ZgcytOIqb77yfckE2s+OJlBgqIyLi0dLtOZxKtXMqze76zz+/0pJO02HfK7TL/g2Azd4NsPT8hPvq1DE5sUjJozIiIiVW8vlsNh4+m6tk/L10nEy1k5HluGi/FpadvOv7MeUtZ8jGi601H6bhnc/i5a2XTJHCoH9ZIlIinUq1023cr5xMtf/rXH8fL8KDbUQGeNEvewY3npuGFSeppSri7DmJJtVbuiGxiOdSGRGREscwDJ6Zu5WTqXbKBdmoFx1MuSCb6yvQRrkgP8oF2Qj/c1uAzRvOHoTZ98PJ9a5v0ugegrq+DrZAc5+MiAdQGRGREmf+lniWbE/E22ph6n3NqRsd8s87bJ4BCx6DrFSwhUD3d6FeL/eEFRGVEREpWU6n2Xnu+20ADLu++j8XkcxkWPA4bP3WNa7YCnpOhNIV3ZBURP5HZURESgzDMHh27jbOZWRTOyqYh9pXz3vy0XUwaxAkHQaLF7R/GtqOBC+9LIq4m/7ViUiJsWBrPIu2JeBttfDWHQ3w9b7EBcmcDvj1bVj+GhgO11GQnpOgohapiphFZURESgTX6ZntADzUIY/TM0lHYfYDcGS1a1z/Duj2Nvj9y5oSESlUKiMiUiI8//12zqZnERsZxLAOlzg9s202/DAC7MngGwjd3oGGd7k9p4hcTGVERIq9BVviWbA1Hi+rhbfuaJj79Iw9DRY9BXFfucblm0GvTyGsqjlhReQiKiMiUqyd+dunZ4a2r0a98n875XL8D9ci1bP7AQtc+5hroaqXjzlhReSSVEZEpFh7bt52zvzv9Mz1NVwbnU5YPQ6WvQTOHAgu7/rIbuW25oYVkUtSGRGRYmvh1ngWbHGdnnnz9j9Pz6ScgDmD4eBK16Tat0D396FUmLlhRSRPKiMiUiydTc/i2bmu0zMPtqtG/ZgQ2Dkf5g2D8+fApxR0fR0a3wsWi8lpReSfqIyISLH0/J+nZ2pFBPHwddEw/1HY8JnrwaiG0GsylK1hbkgRuSwqIyJS7CzeFs8Pm0/gZbXwwfXe2CZ3hNO7XQ+2fgSufxa8fc0NKSKXTWVERIqVc+lZPDN3GxacfFpzAzXnfQyOLAiMhNvGQ7XrzY4oIvmkMiIixcrz87ZD2ilmBHxKi0N/uDbW7Aq3fggBZc0NJyJXRGVERIqNxdsSSNm6gMW2TyjrSAFvP+jyCjQbqEWqIsWYyoiIFAvnklM4O2skU30XuDaE14XbJ0N4bXODichVu8QtLfM2duxYmjdvTlBQEOHh4fTo0YPdu3f/637fffcdsbGx+Pn5Ub9+fRYuXHjFgUXEA53cScbH7fmP4SoiOc0fgPuXqYiIlBD5KiMrVqxg6NCh/P777yxdupTs7Gw6d+5Menp6nvusXr2a3r17M3DgQDZt2kSPHj3o0aMH27Ztu+rwIlLCGQasn4RjQjvK2/dz2ghmf+cpeHd7E3z8zE4nIgXEYhiGcaU7nzp1ivDwcFasWMF11113yTl33XUX6enpzJ8//8K2a665hkaNGjFhwoTL+jkpKSmEhISQnJxMcHDwlcYVkeIk/YzrAma7XUdSlzsasrXZWB6+tY3JwUTkcl3u+/dVrRlJTk4GICws78ssr1mzhpEjR+ba1qVLF+bOnZvnPna7HbvdfmGckpJyNTFFpLjZ/wvMGQJpCeRYfBibdRcrw27nh5uuMTuZiBSCKy4jTqeTESNG0KZNG+rVq5fnvISEBCIiInJti4iIICEhIc99xo4dy5gxY640mogUVzlZrpvbrR4HQEpgVe4+M4hdVGbWHY3w8/EyOaCIFIZ8rRn5u6FDh7Jt2zamT59ekHkAGDVqFMnJyRe+jh49WuA/Q0SKmNP7YPINF4rIjvK30+L0s+wwKvPAddVoXDHU5IAiUliu6MjIsGHDmD9/PitXriQmJuYf50ZGRpKYmJhrW2JiIpGRkXnuY7PZsNlsVxJNRIobw4BNX8KipyA7A8M/lG8in2T0zkoA9G9dmSe61DI5pIgUpnwdGTEMg2HDhjFnzhyWLVtGlSpV/nWfVq1a8fPPP+fatnTpUlq1apW/pCJS8pw/B9/1g3kPQ3YGjkrX8UTZ8YzeWQmLBZ7pVpsXbqmLl1UXNBMpyfJ1ZGTo0KFMmzaN77//nqCgoAvrPkJCQvD39wegb9++lC9fnrFjxwIwfPhw2rVrx9tvv023bt2YPn06GzZsYOLEiQX8VESkWDm0CmY/ACnHwepNettR9NnRkrhjKfh6W3nvrkbcVD/K7JQi4gb5OjIyfvx4kpOTad++PVFRURe+ZsyYcWHOkSNHiI+PvzBu3bo106ZNY+LEiTRs2JCZM2cyd+7cf1z0KiIlmCMbfn4Jpt7sKiJh1TjWax5dNzQl7lgKpUv5MG1QSxUREQ9yVdcZcRddZ0SkhDh7AGbdD8c3uMaN7+GPOk8z4JudJGVkUzGsFFPva07VcoHm5hSRAuGW64yIiFwWw4AtM2DBY5CVBrYQ6P4ei4xWDP88jqwcJw1jQpjcvzllA7V4XcTTqIyISOHKTHaVkK3fucYVW0PPiUzams0rC//AMKBT7QjG9W5EKV+9JIl4Iv3LF5HCc2QtzB4ESUfA4gXtR+Fo8ygvL9rNlN8OAdC3VSWe765PzIh4MpURESl4jhz49W1Y8ToYDihdCXpNJjOyCcO/2cSS7a5rD42+KZb7r62KxaIiIuLJVEZEpGAlHXF9ZPfIGte4wV1w01ucybEx6NPf2XQkCV8vK2/f2ZDuDaPNzSoiRYLKiIgUnG2z4IdHwZ4MvkHQ7W1oeBeHTqfTf8pqDp3JIMTfh0/7NqNFlbxvsCkinkVlRESunj3VdTn3uK9d45jm0PNTCKvCxsPnuP+LDZxNzyIm1J+p97Wgerg+uisif1EZEZGrc3wjzBrkuoaIxQrXPg7tngQvH1bvP82AqevJzHbSICaEyf2aUy5IH90VkdxURkTkyjgd8Nv78Msr4MyB4BjoOREqtwFgzf4zF4pIu5rlGH9PE310V0QuSa8MIpJ/KSdci1QP/eoa17kVur8P/qEArD3wVxFpX6scE+5pip+Pl4mBRaQoUxkRkfzZ+YPrLrvnz4FPKej6BjS+B/78eO66g2e5b+p6zmc7uK6mioiI/DuVERG5PFkZsGQ0bJziGkc1gl6ToWz1C1PWHzpL/ynryMhycG2Nsky8V0VERP6dyoiI/Lv4LTBrIJze4xq3GQ4dngFv3wtTNhw6S//P/ioin/ZtpiIiIpdFZURE8uZ0wtrx8NML4MiCwEi4bQJU65Br2sbDZ+n32TrSsxy0ra4iIiL5ozIiIpeWmghzH4T9P7vGtW6CWz6EgDK5pv1x5Bz9PltPepaD1tXKqIiISL6pjIjIxfYsgbkPQcZp8PaDLq9As4EXFqn+z6Yj5+g3eR1p9hxaVS3D5H7N8fdVERGR/FEZEZG/ZGfC0udg3SeucUQ96DUJwmtfNDXuaBJ9J68j1Z5DyyphTO7fTEVERK6IyoiIuCTucF1J9eR217jlg9DpBfDxu2jq5qNJ3Dt5Lan2HFpUCWPKfc11QTMRuWJ69RDxdIYB6yfBj89ATiYElIMe46HGDZecvuVYEvdMXktqZg4tKocxpb+KiIhcHb2CiHiy9NPw/VDYs9g1rt7JVUQCwy85fdvxZO6Z5CoizSuHMuW+5gTY9DIiIldHryIinmr/MpgzBNISwcsXbngRWgwGq/WS07cdT6bPpLWkZObQtFIoU+5roSIiIgVCryQiniYnC5a9CKs/cI3LxboWqUbWz3OX/xWR5PPZNKlYmqn3NSdQRURECoheTUQ8yem9MHMAJGxxjZsNhM4vg2+pPHfZciyJvp+tI/l8No0rlubzAS0I8vNxU2AR8QQqIyKewDDgjy9g8dOQnQH+YXDrRxB70z/sYvDZb4d4fdEushxOGlVQERGRwqEyIlLSZZyFH4bDznmucZV2cNsnEByV5y6n0+w8/t1mlu8+BcANdSJ4+86GBKuIiEghUBkRKckO/gpzBkPKcbD6QMdnodXDeS5SBfh17ykenbGZ02l2fL2tPHtzHe5pWRHL/7v6qohIQVEZESmJHNmwfCz8+g5gQFg1uH0yRDfOc5esHCdvL93NJysOAFAzIpBxvRsTGxnsptAi4qlURkRKmrMHXFdSPb7RNW58L9z4GtgC89zl8Jl0HvlmE5uPJQPQp2VFnulWR5d3FxG3UBkRKSkMAzZPh4WPQ1Ya+IVA93FQt8c/7jZ303H+O2cr6VkOQvx9eL1XfW6sl/d6EhGRgqYyIlISZCbD/JGwbaZrXKkN9JwIITF57pJmz+G5uduYvek4AC0qh/He3Y2ILu3vjsQiIheojIgUd0fWwuxBkHQELF7QYRS0HQnWvE+xbDmWxCPfbOLQmQysFhjesSbDrq+Ol1WLVEXE/VRGRIorRw78+haseB0MJ5SuBL0mQ4Xmee7idBpMWnWANxbvJsdpEB3ix/u9G9O8cpgbg4uI5KYyIlIcJR2BWffD0d9d4wZ3w01vgl/en3w5mZrJY99u5te9pwHoWi+S13o2IKSUrh0iIuZSGREpbrbOdK0PsSeDbxDc/A40uDPP6ZnZDpbvPsUzc7dyOi0LPx8rz3evy93NK+jaISJSJKiMiBQX9lRY9BTEfe0axzR33eAutHKuaadS7Ww8fI4/jpxjw6GzbDueQpbDCUBsZBAf9G5MjYggN4cXEcmbyohIcXB8o+vaIWcPgMUK1z4O7Z7CafFib0IqGw+fY8Phs/xx+ByHzmRctHvZQF9ua1yexzrXws9H1w4RkaJFZUSkKHM64Lf34ZdXwJmDMziG7de8xfLz1dnw+R/8ceQcqZk5uXaxWKBmeBBNKoXSrFIozSqHUjGslE7JiEiRpTIiUlQlH3fdV+bQrwCs8m3Lw6f7cW6eE9hzYVopXy8aVShN00qhNK0USuOKoYT4a1GqiBQfKiMiRdGOeRjzHsaSmUQGfjyf3ZfvMtsBFqJC/Gj651GPppXCqB0VhLdX3je+ExEp6lRGRIqSrHSyF47CJ+5zLMBmZ1WGZw/FGVqNV9pVpUOtcF0hVURKHJURkSIi7dBGsmfcR+j5wzgNC584bmZWcF+GdaxDj0bROvohIiWWyoiIyZIz7Gz+7lVaHfyAQBwkGKG8WWokbTv3ZHEDlRARKflURkRMkpSRxYxfNlBv/dNcx2YAVnlfQ0rnd3ijWR3dJ0ZEPIbKiIibnUvPYvKqgxxcPYsX+ZgyllQy8WVPo9G07j4cq46EiIiHURkRcZOz6Vl8+usBZqzew3DnFzzuvRSAlJBYAv/zOQ0iYk1OKCJiDpURETfYcSKFuyauIdp+gG98PqSW9zEAjJYPEXzDC+BtMzegiIiJVEZECllWjpORMzZxW/YCnrFNw5dsjIByWHpMwFKjk9nxRERMpzIiUsg+XbyOJ84+T0efTa4N1W/A0uNjCAw3N5iISBGhMiJSiPavmccd6x8m3CsJh9UXr84vQcvBrhvIiIgIoDIiUjhy7OQsfYFqaz8GC8T7ViZqwNcQWc/sZCIiRY7KiEhBO7UHZg3AO2ErAN9Zb6TTQxOhdIjJwUREiiZd0ECkoBgGbJwKn1wHCVs5awQyKOsxQm8fR6iKiIhInnRkRKQgZJyFeQ/DrvkAbPBqyEPpD3Btk/p0qhNhcjgRkaJNZUTkah1cCbMHQ+oJsPrwY9RgBu+/hojgUjzXvY7Z6UREijyVEZEr5ciGX16FVe8CBpSpzpaW7/DA7DQAXr+9ASH+PuZmFBEpBrRmRORKnNkPkzvDqncAA5r0Ja3/Mh76xQFA7xYVaVeznLkZRUSKCR0ZEckPw4DN38DCJyArDfxKwy3joM6tjJ2zlWPnzlO+tD//7Vbb7KQiIsWGyojI5TqfBAtGwrZZrnGlttDzEwiJ4de9p/h67REA3ryjAYE2/dMSEblcesUUuRxHfodZ90PyEbB4QYfR0PZRsHqRkpnNkzO3ANCvVSVaVytrclgRkeJFZUTknzhyYOWbsPINMJwQWhl6TYaYZhemvPTDDuKTM6lcphRPdY01L6uISDGlMiKSl3OHYfb9cHSta9ywN3R9A/yCL0z5eWci3208hsUCb93RkFK++iclIpJfeuUUuZStM2H+o2BPAVswdHsHGtyRa0pSRhajZrsu+T6obRWaVQ4zI6mISLGnMiLyd/ZU1ydlNn/jGldoCT0nuk7P/D8vzNvOyVQ71coF8FjnWu7NKSJSgqiMiPzPsY0wayCcOwgWK1z3JFz3BHhd/M9k8bYE5sadwPrn6Rk/Hy8TAouIlAwqIyJOh+sqqsvHgjMHQipAz0+hUqtLTj+TZue/c1ynZ4a0q0bjiqHuTCsiUuKojIhnSz7muq/M4VWucd2ecPO74F/6ktMNw+DZ77dxJj2L2Mgghneq4b6sIiIllMqIeK4d38O8RyAzCXwCoNtbrk/MWCx57jJ/SzwLtybgbbXw1h0NsXnr9IyIyNXK971pVq5cSffu3YmOjsZisTB37tx/nL98+XIsFstFXwkJCVeaWeTqZKXDvIfh276uIhLdGIb8Co3+849F5GRqJs9+vw2AYddXp175EDcFFhEp2fJ9ZCQ9PZ2GDRsyYMAAevbsedn77d69m+Dgv67PEB4ent8fLXL1TsS5Fqme2QdYoO0IaD8avH1zTct2ODl4Op2d8SnsjE9lV0IKW48lk5SRTd3oYIZ2qG5GehGREinfZaRr16507do13z8oPDyc0qVL53s/kQLhdMLvH8FPY8CZDUHRrvvKVLmOs+lZ7Dx0Olfx2JuYRpbDedG3KRPgy9t3NsTHSze8FhEpKG5bM9KoUSPsdjv16tXjhRdeoE2bNu760eLpUhNgzhA48AsAJ6I68W3UE/zxi5Vd037iZKr9krsF+HoRGxVM7aggYiODqf3nf9dVVkVEClahv6pGRUUxYcIEmjVrht1uZ9KkSbRv3561a9fSpEmTS+5jt9ux2/96g0hJSSnsmFJS7V4E3w+FjDNkW208Z7+Hbw5eDwfP5JpWqUwpakcGExsV5CodkcHEhPpjtea9hkRERApGoZeRWrVqUavWX1enbN26Nfv37+fdd9/lyy+/vOQ+Y8eOZcyYMYUdTUqy7PPw47Ow/lMAEkvV5D/n7me/UZ7GFUtTN9p1pCM2MpjYyCACbDraISJiFlNegVu0aMGqVavyfHzUqFGMHDnywjglJYUKFSq4I5qUBAnbYNYgOLUTgDURvel3+Eay8OHprrEMaVfN5IAiIvJ3ppSRuLg4oqKi8nzcZrNhs9ncmEhKBMOAdRNdR0QcdoyAcD6PeJoXdkQCMOaWuvRrXdncjCIicpF8l5G0tDT27dt3YXzw4EHi4uIICwujYsWKjBo1iuPHj/PFF18A8N5771GlShXq1q1LZmYmkyZNYtmyZfz4448F9yxE0k7B9w/BXtfflVGjC2OsDzF1czoWC4y9rT53t6hockgREbmUfJeRDRs20KFDhwvj/51O6devH1OnTiU+Pp4jR45ceDwrK4vHHnuM48ePU6pUKRo0aMBPP/2U63uIXJV9P8GcByH9JHjZcNzwEiMONOeHzfF4WS28fUdDejQub3ZKERHJg8UwDMPsEP8mJSWFkJAQkpOTc104TTxcjt113ZDfP3KNw+uQ1WMiD/9sZ8n2RLytFj7o3Ziu9fM+JSgiIoXnct+/9RECKZ5O7YaZAyHRdfdcWjxAZvvnGTJjB8t3n8LX28qEe5pwfWyEuTlFRORfqYxI8WIYsHEKLB4NOeehVBm49WPSK3di0OcbWHPgDH4+Vib1bU7bGmXNTisiIpdBZUSKj4yzrhvc7ZrvGle7HnqMJ8WnDAM+W8eGw+cI8PXis/7NaVm1jLlZRUTksqmMSPFwYAXMGQyp8WD1gU4vwDUPkZSZQ99Ja9lyLJlgP28+H9CCxhVDzU4rIiL5oDIiRVtOFvzyCvz2PmBAmRpw+2SIasjpNDv3TFrLroRUQkv58OXAltQrH2J2YhERySeVESm6zuyHWQPhxCbXuGl/6PIq+AaQmJJJn0lr2XcyjbKBNqbd35KaEUGmxhURkSujMiJFj2FA3DRY+ARkp4NfabjlA6hzCwDHzmXQZ9JaDp/JICrEj68HtaRquUBzM4uIyBVTGZGi5XwSzH8Uts92jStfC7d9AiGui5YdPpPOfz5dy/Gk81QI82faoGuoEFbKvLwiInLVVEak6Di8BmbfD8lHweoNHUZDmxFg9QJgT2Iq90xay8lUO1XLBvD1/S2JCvE3N7OIiFw1lRExnyMHVr4BK98EwwmhVaDXZIhpemFK3NEk+k9ZR1JGNrUigvhyUAvCg/xMDC0iIgVFZUTMde4QzH4Ajq51jRv+B256A2x/LUZdve8093+xgfQsB40qlGbqfc0pXcrXnLwiIlLgVEbEPFtnutaH2FPAFgw3vwv1b8815cftCQz7ZhNZOU7aVC/DxHubEWDTn62ISEmiV3VxP3uq65Mym79xjSu0hJ6fQmilXNNm/3GMJ2ZuweE06FI3gnG9G2Pz9jIhsIiIFCaVEXGvYxth1gDX6RmLFa57Eq57Arxy/ylO/e0gL/ywA4BeTWJ4vVd9vL2sJgQWEZHCpjIi7uF0wKp3YflYcOZASEXoOREqtco1zTAMPli2j3eW7gHgvjaVebZbHaxWixmpRUTEDVRGpPAlH4PZg+HwKte4Xi/o9g74l841zTAMXlmwk0mrDgIwolMNhnesgcWiIiIiUpKpjEjh2j4XfhgOmUngGwg3vQUN74b/VzAcToNRs7fw7YZjADx3cx0GtK3i/rwiIuJ2KiNSOLLSYfHT8McXrnF0E+g1CcpUu2iqPcfBiOlxLNqWgNUCb9zekNubxrg5sIiImEVlRAreiTjXDe7O7AMs0PZR19VUvXwumpqRlcPgLzfy697T+HpZGde7MTfWi3R7ZBERMY/KiBQcpxPWfAg/vwjObAiKdi1SrXLtJacnZ2Qz4PP1bDx8jlK+Xky8txlta5R1c2gRETGbyogUjNQEmDMYDix3jWt3h+7joFTYJaefSrVz7+S17EpIJdjPm6kDWtCkYqj78oqISJGhMiJXb/ci+H4oZJwBn1Jw41ho0u+iRar/c+xcBvdMWsuhMxmUC7Lx5cAWxEYGuzm0iIgUFSojcuWyz8OPz8D6Sa5xZAPXDe7K1cxzl42HzzJs2ibikzOJCfXnq4EtqVw2wE2BRUSkKFIZkSuTsM21SPXULte41TDo+Bx42y45/WRqJq8t2sXsP44DUD08kK8GtiQyRHfeFRHxdCojkj+GAWs/gaXPgcMOgRHQYzxU73jJ6dkOJ5+vPsR7P+0lzZ4DwF3NKjDqpljdeVdERACVEcmPtFPw/UOw90fXuOaNcOtHEHDpT8D8tu80L8zbzt6TaQA0jAlhzK31aFShtJsCi4hIcaAyIpdn708wdwiknwIvG3R+GVrcf8lFqseTzvPqgp0s2BoPQFiAL0/dWIs7mlbQPWZEROQiKiPyz3Ls8NML8PvHrnF4Hdci1Yg6F03NzHYw6dcDfPTLfs5nO7Ba4N5rKjHyhlqElLr4gmciIiKgMiL/5OQumDUIEre6xi0Gww1jwMf/oqnLdiUy5ocdHD6T4ZpaOYwXbqlLnWh9ZFdERP6ZyohczDBg4xRYPBpyzkOpMq5FqjW7XDT10Ol0Xpy/g2W7TgIQEWxj9E21uaVhtO62KyIil0VlRHJLPwPzHobdC1zjatdDjwkQFJFrWkZWDh//sp+JKw+Q5XDi42VhQNsqPHx9DQJt+rMSEZHLp3cN+cuB5TBnCKTGY1h9SGrzXxLr3EfaGSdpJ06SbneQZs/mXEY2X6w+xInkTACurVGW57vXpXp4oLn5RUSkWFIZ8UCHTqfz8fJ9nEq1k2bPITMzk7vTvqR3zhysGOxzRjPcPoztSyvD0t/y/D7lS/vzXPc6dK4ToVMyIiJyxVRGPMz5LAcDPl/PgVPpAFSxxPO+z4c0sB4E4OucjryUcw+Z2PDzsRJo8ybA5p3rPwNt3tSJDqZ/68r4+XiZ+XRERKQEUBnxMK8v3sWBU+mEB/ryUZ0dNN7+Gt6ODHJspTnZ/k3a1+7Ozb7eBNi88Paymh1XREQ8gMqIB1m97zRTVx8imDTmR88lfMtC1wOVr8X7tk+IDilvbkAREfFIKiMeIiUzm8e/20xzyy4+DZxA6SMnweoN1z8DrR8Bq063iIiIOVRGPMTL87ZwV/qXDLN9j1e2E8KqQs9JENPU7GgiIuLhVEY8wK/r1nPXtgdp6r3XtaFRH+j6OtiCzA0mIiKCykiJl7ruaxovfIxA63kyvQLx6/E+1L/d7FgiIiIXqIyUVJkpGAsfI2jLtwBs86pNjQenQ9nK5uYSERH5f/TZzZLo6HqY0BbLlm/JMay877gdy30LsKmIiIhIEaQjIyWJ0wG/vgPLx4Lh4JhRjkeyhtKxc3fqxpQxO52IiMglqYyUFElHYc5gOOy6fPtq//YMPncP1StGM/i6qiaHExERyZvKSEmwfS788AhkJoNvIL/WfJp7N1TBz8eLt+9oqCupiohIkaZ3qeLMngbfD4Pv+rmKSPmmHLlzCfdvrg5YGNW1NlXL6U66IiJStKmMFFcnNsHEdrDpS8AC1z5OTr9FPLIkmcxsJ22rl+XeayqZnVJERORf6TRNceN0wpoP4OeXwJkNweWh50So3JYJy/YSdzSJID9v3ri9AVarxey0IiIi/0plpDhJiXctUj24wjWu3R26j4NSYWw7nsx7P7musDrmlrpEl/Y3MaiIiMjlUxkpLnYtcK0POX8WfErBja9Bk75gsWDPcfDYt5vJcRrcWDeS2xrr7rsiIlJ8qIwUdVkZ8OMzsGGyaxzZAG7/DMrWuDDlnaV72J2YStlAX165rR4Wi07PiIhI8aEyUpQlbINZA+HULte49cNw/bPgbbswZf2hs0xceQCAV2+rT5lA26W+k4iISJGlMlIUGQasnQBLnwNHFgRGwG0ToNr1uaal23N47NvNGAbc3jSGznUjTQosIiJy5VRGipq0kzD3Idi31DWu2RVu/RACyl409ZWFOzlyNoPypf15rnsdNwcVEREpGCojRcnepTD3QUg/Bd5+0PllaD4ILrEG5JfdJ5m29ggAb97RgGA/H3enFRERKRAqI0VBdib89AKsHe8ah9eBXpMhIvfRDqfTYMPhcyzcGs/sP44BcF+byrSudvFRExERkeJCZcRsJ3e5FqkmbnONWw6BTmPAxw8Ah9Ng/aGzLNoaz6JtCZxMtV/YtW50ME/dGGtGahERkQKjMmIWw3B9XHfJfyEnE0qVhR4fQ80uOJwGa/efZtHWBBZvT+DU3wpIkJ83N9SJoFv9KNrWKIvN28vEJyEiInL1VEbMkH4G5j0Muxe4xtU6knPLR6w95cPCOVtZsj2B02lZF6YH+3nTuW4k3epH0bp6GRUQEREpUVRG3O3Acpg9GNISMLx8OdDoCSZndWHJuG2cSf+rgJQu5UPnOhHcVD+K1tXK4uutexqKiEjJpDLiLjlZsOwlWP0BYHDGvzIPZT7E2t9iANdi1NBSPnSpG8lN9aNoVa0MPl4qICIiUvKpjLjD6X2uRarxcQB8SyeeO9eHTGyEBfjS5c9TMNdUDcNbBURERDyMykhhMgzY9BXGoiexZGeQRCBPZt3Pj87m1IwIZOQNtehUO1wFREREPJrKSGE5fw7nvOFYd36PBVjtqMOj2Q9hC4vhvRtq0r1hNF5W3dBOREREZaQQOA+uwv7tIPzPx5NtePF2zh3M9e/Jw91iuat5Ba0FERER+RuVkQJk5GRxeM7zVNw+Hn8MDjojeMZrBO26dGF5q8r4+egjuSIiIv+fykgB2bI1Dr95g6mZvQuAWUYH4lu/wIT2dQnSfWNERETypDJylbYdT+a32R/xn9PjCLKcJ8UoxU/VR9Gh5xDCAnzNjiciIlLk5XvxwsqVK+nevTvR0dFYLBbmzp37r/ssX76cJk2aYLPZqF69OlOnTr2CqEXL+SwHT339K3sn9GbwmdcJspznUEADMgetoOe9j6iIiIiIXKZ8l5H09HQaNmzIRx99dFnzDx48SLdu3ejQoQNxcXGMGDGCQYMGsWTJknyHLSoMw2D8V98wdPd93Ob1Gw6snGv5JJUfW054hZpmxxMRESlWLIZhGFe8s8XCnDlz6NGjR55znnrqKRYsWMC2bdsubLv77rtJSkpi8eLFl/VzUlJSCAkJITk5meDg4CuNWzCcDtZ9+V+aHPgEb4uTzIAY/O6eAhVamJtLRESkiLnc9+9CXzOyZs0aOnXqlGtbly5dGDFiRJ772O127Pa/7lSbkpJSWPHyJ+koSV/1p8XpDWCBA1HdqNpvPPiFmJ1MRESk2Cr0C14kJCQQERGRa1tERAQpKSmcP3/+kvuMHTuWkJCQC18VKlQo7Jj/bttsHB+3pvTpDaQa/syo8CxVHvhaRUREROQqFcmrb40aNYrk5OQLX0ePHjUvjD0N5g6FmffhlZXCJmd1nir7Ebf1G4nFoiuoioiIXK1CP00TGRlJYmJirm2JiYkEBwfj7+9/yX1sNhs2m62wo/2743/ArEFwdj9OLHyYcyvflfoPs/u3x9e7SPY4ERGRYqfQy0irVq1YuHBhrm1Lly6lVatWhf2jr5zTCavfh2UvgzOHFN9wBqUOZrNXXWb1u4ZyQUWgKImIiJQQ+S4jaWlp7Nu378L44MGDxMXFERYWRsWKFRk1ahTHjx/niy++AGDIkCF8+OGHPPnkkwwYMIBly5bx7bffsmDBgoJ7FgUp5QTMGQwHVwJwPLozXQ/cTgqBvH97A+qV1xoRERGRgpTvMrJhwwY6dOhwYTxy5EgA+vXrx9SpU4mPj+fIkSMXHq9SpQoLFizg0Ucf5f333ycmJoZJkybRpUuXAohfwHbOh3nD4Pw58CnFsVZj6PRLDJkYDGlXjVsblTc7oYiISIlzVdcZcZdCv85IVgYsGQ0bp7jGUY0423U8N38dz4nkTNrXKsfkfs3xsmrBqoiIyOW63PdvrcKM3wIT2/9VRNoMJ6v/EoYsTOZEciZVywbw/t2NVUREREQKiefeKM/phLXj4acXwJEFgZFw2wSo1oEX5mxl3aGzBNm8mdi3GSH+uuuuiIhIYfHcMuKww8bPXUWk1k1wy4cQUIavfj/MtLVHsFhgXO/GVA8PNDupiIhIiea5ZcTHH26fDEfXQrOBYLGw9sAZXpi3HYAnu8TSITbc5JAiIiIln+eWEYDI+q4v4Ni5DB76+g9ynAbdG0YzpF1Vk8OJiIh4Bi1gBc5nOXjgi42cSc+ibnQwb/RqoEu9i4iIuInHlxHDMHhi5mZ2xKdQJsCXiX2b4e/rZXYsERERj+HxZWT8iv3M3xKPt9XC+HuaUr70pe+XIyIiIoXDo8vIzzsTeXPJbgDG3FqXFlXCTE4kIiLieTy2jGRk5fDEzC0YBvRpWZE+LSuZHUlERMQjeWwZKeXrzad9m9KtQRTPd69rdhwRERGP5dEf7W1aKYymlXRqRkRExEwee2REREREigaVERERETGVyoiIiIiYSmVERERETKUyIiIiIqZSGRERERFTqYyIiIiIqVRGRERExFQqIyIiImIqlRERERExlcqIiIiImEplREREREylMiIiIiKmKhZ37TUMA4CUlBSTk4iIiMjl+t/79v/ex/NSLMpIamoqABUqVDA5iYiIiORXamoqISEheT5uMf6trhQBTqeTEydOEBQUhMViKbDvm5KSQoUKFTh69CjBwcEF9n2LE0//HXj68wf9DvT8Pfv5g34Hhfn8DcMgNTWV6OhorNa8V4YUiyMjVquVmJiYQvv+wcHBHvkH+Hee/jvw9OcP+h3o+Xv28wf9Dgrr+f/TEZH/0QJWERERMZXKiIiIiJjKo8uIzWbj+eefx2azmR3FNJ7+O/D05w/6Hej5e/bzB/0OisLzLxYLWEVERKTk8ugjIyIiImI+lRERERExlcqIiIiImEplREREREzlkWVk/PjxNGjQ4MIFXlq1asWiRYvMjmWa1157DYvFwogRI8yO4jYvvPACFosl11dsbKzZsdzq+PHj3HPPPZQpUwZ/f3/q16/Phg0bzI7lNpUrV77ob8BisTB06FCzo7mFw+Hg2WefpUqVKvj7+1OtWjVeeumlf72HSEmSmprKiBEjqFSpEv7+/rRu3Zr169ebHavQrFy5ku7duxMdHY3FYmHu3Lm5HjcMg+eee46oqCj8/f3p1KkTe/fudUs2jywjMTExvPbaa2zcuJENGzZw/fXXc+utt7J9+3azo7nd+vXr+eSTT2jQoIHZUdyubt26xMfHX/hatWqV2ZHc5ty5c7Rp0wYfHx8WLVrEjh07ePvttwkNDTU7mtusX78+1//+S5cuBeCOO+4wOZl7vP7664wfP54PP/yQnTt38vrrr/PGG2/wwQcfmB3NbQYNGsTSpUv58ssv2bp1K507d6ZTp04cP37c7GiFIj09nYYNG/LRRx9d8vE33niDcePGMWHCBNauXUtAQABdunQhMzOz8MMZYhiGYYSGhhqTJk0yO4ZbpaamGjVq1DCWLl1qtGvXzhg+fLjZkdzm+eefNxo2bGh2DNM89dRTRtu2bc2OUaQMHz7cqFatmuF0Os2O4hbdunUzBgwYkGtbz549jT59+piUyL0yMjIMLy8vY/78+bm2N2nSxPjvf/9rUir3AYw5c+ZcGDudTiMyMtJ48803L2xLSkoybDab8c033xR6Ho88MvJ3DoeD6dOnk56eTqtWrcyO41ZDhw6lW7dudOrUyewopti7dy/R0dFUrVqVPn36cOTIEbMjuc28efNo1qwZd9xxB+Hh4TRu3JhPP/3U7FimycrK4quvvmLAgAEFejPOoqx169b8/PPP7NmzB4DNmzezatUqunbtanIy98jJycHhcODn55dru7+/v0cdJf2fgwcPkpCQkOv9ICQkhJYtW7JmzZpC//nF4kZ5hWHr1q20atWKzMxMAgMDmTNnDnXq1DE7lttMnz6dP/74o0SfH/0nLVu2ZOrUqdSqVYv4+HjGjBnDtddey7Zt2wgKCjI7XqE7cOAA48ePZ+TIkYwePZr169fzyCOP4OvrS79+/cyO53Zz584lKSmJ/v37mx3FbZ5++mlSUlKIjY3Fy8sLh8PBK6+8Qp8+fcyO5hZBQUG0atWKl156idq1axMREcE333zDmjVrqF69utnx3C4hIQGAiIiIXNsjIiIuPFaYPLaM1KpVi7i4OJKTk5k5cyb9+vVjxYoVHlFIjh49yvDhw1m6dOlF/6/AU/z9//01aNCAli1bUqlSJb799lsGDhxoYjL3cDqdNGvWjFdffRWAxo0bs23bNiZMmOCRZWTy5Ml07dqV6Ohos6O4zbfffsvXX3/NtGnTqFu3LnFxcYwYMYLo6GiP+Rv48ssvGTBgAOXLl8fLy4smTZrQu3dvNm7caHY0j+Oxp2l8fX2pXr06TZs2ZezYsTRs2JD333/f7FhusXHjRk6ePEmTJk3w9vbG29ubFStWMG7cOLy9vXE4HGZHdLvSpUtTs2ZN9u3bZ3YUt4iKirqoeNeuXdujTlX9z+HDh/npp58YNGiQ2VHc6oknnuDpp5/m7rvvpn79+tx77708+uijjB071uxoblOtWjVWrFhBWloaR48eZd26dWRnZ1O1alWzo7ldZGQkAImJibm2JyYmXnisMHlsGfn/nE4ndrvd7Bhu0bFjR7Zu3UpcXNyFr2bNmtGnTx/i4uLw8vIyO6LbpaWlsX//fqKiosyO4hZt2rRh9+7dubbt2bOHSpUqmZTIPFOmTCE8PJxu3bqZHcWtMjIysFpzvwV4eXnhdDpNSmSegIAAoqKiOHfuHEuWLOHWW281O5LbValShcjISH7++ecL21JSUli7dq1b1lN65GmaUaNG0bVrVypWrEhqairTpk1j+fLlLFmyxOxobhEUFES9evVybQsICKBMmTIXbS+pHn/8cbp3706lSpU4ceIEzz//PF5eXvTu3dvsaG7x6KOP0rp1a1599VXuvPNO1q1bx8SJE5k4caLZ0dzK6XQyZcoU+vXrh7e3Z70cdu/enVdeeYWKFStSt25dNm3axDvvvMOAAQPMjuY2S5YswTAMatWqxb59+3jiiSeIjY3lvvvuMztaoUhLS8t19PfgwYPExcURFhZGxYoVGTFiBC+//DI1atSgSpUqPPvss0RHR9OjR4/CD1fon9cpggYMGGBUqlTJ8PX1NcqVK2d07NjR+PHHH82OZSpP+2jvXXfdZURFRRm+vr5G+fLljbvuusvYt2+f2bHc6ocffjDq1atn2Gw2IzY21pg4caLZkdxuyZIlBmDs3r3b7Chul5KSYgwfPtyoWLGi4efnZ1StWtX473//a9jtdrOjuc2MGTOMqlWrGr6+vkZkZKQxdOhQIykpyexYheaXX34xgIu++vXrZxiG6+O9zz77rBEREWHYbDajY8eObvu3YTEMD7rcnoiIiBQ5WjMiIiIiplIZEREREVOpjIiIiIipVEZERETEVCojIiIiYiqVERERETGVyoiIiIiYSmVERERETKUyIiIiIqZSGRERERFTqYyIiIiIqVRGRERExFT/B4xGZ6iJ6BfwAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.plot(X, lm.predict(X).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4304565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6658, dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5e841f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6000, 0.6483, 0.6966, 0.7448, 0.7931, 0.8414, 0.8897, 0.9379, 0.9862,\n",
       "        1.0345, 1.0828, 1.1310, 1.1793, 1.2276, 1.2759, 1.3241, 1.3724, 1.4207,\n",
       "        1.4690, 1.5172, 1.5655, 1.6138, 1.6621, 1.7103, 1.7586, 1.8069, 1.8552,\n",
       "        1.9034, 1.9517, 2.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4948fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3.0)\n",
    "b = torch.tensor(3.0)\n",
    "a.requires_grad = True\n",
    "print(a.is_leaf)\n",
    "print(b.is_leaf)\n",
    "\n",
    "c = a * b\n",
    "print(c.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48b5aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1737.90234375\n",
      "199 37.529075622558594\n",
      "299 1.5820496082305908\n",
      "399 0.0880310907959938\n",
      "499 0.0060176001861691475\n"
     ]
    }
   ],
   "source": [
    "n, input_dim, hidden_dim, output_dim = 64, 784, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(n, input_dim)\n",
    "y = torch.randn(n, output_dim)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(input_dim, hidden_dim)\n",
    "w2 = torch.randn(hidden_dim, output_dim)\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x @ w1\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu @ w2\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "        \n",
    "        \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ab5893d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1415,  0.3617, -1.5502,  ...,  0.2441, -0.1792, -0.0707],\n",
       "         [ 0.7776,  1.5002, -0.3116,  ...,  1.3174, -0.4838,  0.0572],\n",
       "         [-0.2551,  0.4376,  0.3212,  ...,  0.9126,  0.4178, -0.6002],\n",
       "         ...,\n",
       "         [-0.9394, -0.1692, -0.1052,  ...,  0.7911,  1.2300,  0.4635],\n",
       "         [ 1.8118,  0.2694,  0.0475,  ...,  0.7771,  1.3587, -1.0109],\n",
       "         [-1.4929,  0.5852,  0.6510,  ...,  1.0493,  0.9050,  2.4599]]),\n",
       " tensor([[-7.6154e-01, -9.6260e-04,  7.2759e-02, -1.2996e+00,  7.2385e-01,\n",
       "           1.4477e+00,  4.4407e-01, -1.3210e-01,  1.2482e-01, -1.0593e-01],\n",
       "         [ 3.8737e-01,  5.6627e-01, -8.4857e-01, -8.6267e-01, -2.3556e+00,\n",
       "           1.2945e+00, -1.8677e+00, -3.0032e-01, -1.6406e+00, -8.1563e-01],\n",
       "         [ 2.5713e-01, -2.4955e-01, -1.1174e+00,  1.6269e+00,  2.7387e-01,\n",
       "          -3.0364e-01,  7.4902e-01, -9.6764e-01,  1.1022e+00,  1.0981e+00],\n",
       "         [ 4.1538e-01,  8.5702e-01, -6.1458e-01,  1.4468e-01, -2.1006e-01,\n",
       "           6.3491e-01, -3.2266e-01, -8.7106e-01,  3.0148e-01, -1.1419e+00],\n",
       "         [-4.2875e-02,  1.6276e+00, -1.1649e-01, -4.4837e-01, -1.1179e+00,\n",
       "          -1.5953e-01, -6.3446e-01,  1.5911e+00, -3.2135e-01, -5.5514e-01],\n",
       "         [ 5.2168e-01, -1.4768e+00,  3.4975e-01, -9.2118e-01, -3.0800e+00,\n",
       "          -8.4307e-01, -2.7143e-01,  1.8308e-01,  2.7944e-01,  1.1614e-01],\n",
       "         [-1.5501e-01, -1.8493e+00, -3.1051e-02,  6.3985e-01,  3.4319e-01,\n",
       "          -1.7188e-01, -2.4293e-01, -1.3249e-01, -1.1239e-01, -8.4996e-01],\n",
       "         [ 1.3739e+00, -6.7778e-02,  5.6446e-01, -1.8795e+00, -1.4603e+00,\n",
       "          -9.1210e-01,  6.6238e-01, -2.9047e-01, -6.2303e-01,  2.5496e-01],\n",
       "         [ 1.0022e-01,  2.5152e-01,  1.7817e-01,  5.8591e-01,  2.7279e-01,\n",
       "          -8.0337e-01,  8.1936e-01,  2.9827e-01,  1.8296e-01,  8.1867e-01],\n",
       "         [-1.3630e+00,  9.0639e-01,  2.1946e+00, -2.8982e+00, -2.0436e+00,\n",
       "           2.3442e+00, -1.2402e+00, -9.7046e-01,  1.6150e+00, -1.3447e-01],\n",
       "         [-4.3642e-01, -8.5381e-01, -4.5822e-01, -1.5405e+00,  1.5463e+00,\n",
       "          -2.1584e+00,  1.2899e+00,  8.7596e-01, -2.1842e-01, -1.6291e+00],\n",
       "         [-1.2103e+00,  2.6382e-01, -8.7222e-02, -6.5652e-01,  4.8737e-01,\n",
       "           5.0315e-01, -7.8091e-01,  3.3236e-01,  1.2428e-01, -1.4221e+00],\n",
       "         [ 4.1761e-01,  8.7979e-01, -1.3030e+00,  1.9734e-01,  9.2917e-01,\n",
       "          -7.0875e-01,  1.1578e+00, -7.3674e-01,  6.4746e-01,  8.5042e-01],\n",
       "         [-1.3573e+00,  8.2501e-01, -8.0160e-01, -1.1096e+00,  2.1018e+00,\n",
       "           2.4987e-01, -6.3905e-01, -6.1658e-01,  1.9935e+00, -4.0872e-01],\n",
       "         [ 9.0476e-01,  1.2628e+00, -9.3921e-02,  1.2113e+00,  7.7313e-01,\n",
       "          -5.5433e-01, -1.4099e+00, -7.5600e-02,  3.8216e-01, -9.6561e-01],\n",
       "         [-4.5729e-01,  6.6607e-01, -9.7910e-01,  1.0129e-01,  5.7990e-01,\n",
       "          -4.3938e-01,  3.6568e-01, -6.7925e-01,  1.6220e+00,  1.2303e+00],\n",
       "         [ 6.8643e-02, -9.4019e-01,  3.3555e-01,  9.8670e-01,  1.1996e+00,\n",
       "           2.6204e-01,  9.8570e-02, -1.0347e+00, -1.5759e+00,  4.3262e-01],\n",
       "         [-1.5254e-01, -1.1937e+00,  7.1002e-01,  5.1069e-01,  5.6445e-02,\n",
       "          -1.3548e+00, -1.3048e-01, -1.0426e+00,  2.8437e-02, -2.0508e+00],\n",
       "         [-7.9622e-01, -3.6502e-01,  4.5078e-01, -5.3663e-01,  4.1622e-02,\n",
       "          -1.0891e+00, -1.6207e+00, -8.8094e-01,  5.6826e-01,  6.2207e-02],\n",
       "         [-1.2777e+00,  1.0026e+00, -2.6598e-01,  2.1397e+00, -8.6539e-01,\n",
       "          -3.0592e-01, -6.4492e-01,  1.0553e+00, -1.2774e+00,  7.8849e-01],\n",
       "         [ 4.2808e-01,  1.1602e-01, -6.5572e-01,  7.3008e-01,  8.1338e-01,\n",
       "          -2.1771e-01,  2.8812e-01,  1.8979e-01, -3.8374e-01,  9.5924e-01],\n",
       "         [-3.0740e-01,  1.1354e+00,  9.1285e-01,  2.4037e+00, -9.3414e-02,\n",
       "           6.7359e-01,  2.2131e+00,  1.5595e-01,  3.4668e-01, -4.8701e-01],\n",
       "         [-5.2998e-02,  9.1401e-01,  2.5210e-02,  2.9303e-01,  2.4846e-01,\n",
       "          -7.9181e-01, -1.3613e-01,  8.9051e-01,  1.1406e+00,  6.1142e-01],\n",
       "         [-1.0872e+00, -8.7320e-01,  1.4845e+00, -4.8752e-03, -8.2446e-01,\n",
       "          -7.1959e-01,  4.9925e-01, -6.5923e-01,  4.0342e-05, -1.9623e+00],\n",
       "         [-2.0883e+00, -2.3415e-01, -6.2383e-01,  9.9762e-01, -1.0630e+00,\n",
       "           6.3019e-01, -1.0933e-01, -1.3333e-01, -2.1780e+00,  2.2020e+00],\n",
       "         [-5.0971e-01, -1.5106e+00, -1.0571e+00, -2.0935e+00,  1.1551e+00,\n",
       "          -3.9403e-01, -2.2684e-01, -6.8379e-02, -8.6634e-01, -1.1380e+00],\n",
       "         [ 9.6394e-01,  9.5941e-01,  1.5155e-01, -2.4175e-01, -1.7309e+00,\n",
       "          -4.4962e-01,  7.3684e-01, -3.8380e-01, -4.6207e-01,  3.7774e-02],\n",
       "         [ 1.4184e+00,  1.7657e-01, -1.0617e-01,  1.2796e+00, -1.2051e+00,\n",
       "           5.5549e-01,  6.4807e-01,  8.4187e-02, -2.3592e+00,  3.9724e-01],\n",
       "         [-3.5847e-01, -8.7733e-01,  5.0942e-01, -9.5951e-01, -1.0555e-01,\n",
       "          -2.1509e+00,  1.5364e+00, -7.1032e-02, -1.7788e+00, -4.0551e-01],\n",
       "         [-6.7955e-01, -8.3731e-01,  2.0354e+00,  5.5203e-01, -5.7687e-01,\n",
       "          -4.1570e-01, -1.2720e+00,  3.0259e-01,  2.0839e-02,  1.2084e+00],\n",
       "         [ 9.8856e-01,  5.2397e-01,  2.3164e-01,  2.7616e-01, -1.9241e-01,\n",
       "          -4.3172e-01,  6.5872e-01, -3.6624e-01,  1.0606e+00,  2.0541e+00],\n",
       "         [ 1.1308e+00, -1.7963e-01, -9.9700e-01,  1.0625e-01, -2.2971e-01,\n",
       "          -4.0511e-01,  4.3673e-01,  1.1767e+00, -7.5279e-02,  2.3334e-02],\n",
       "         [ 3.9940e-02,  4.1079e-01,  6.0778e-01, -7.3801e-01,  4.2302e-01,\n",
       "           1.2351e-01,  1.1070e+00,  2.4267e-01, -2.0130e-02, -1.7809e-01],\n",
       "         [ 2.4476e-01,  1.0653e+00,  1.2162e+00, -2.1195e-01, -2.0247e+00,\n",
       "          -1.5803e-01, -6.4936e-01,  2.4144e-02,  4.7192e-01,  1.4388e+00],\n",
       "         [-1.6657e+00,  4.9271e-01, -3.9025e-01,  1.3422e-01, -3.5367e-01,\n",
       "           3.4111e+00,  4.3444e-01,  5.0713e-01,  6.7926e-02,  2.9787e-01],\n",
       "         [ 9.4398e-01,  7.0948e-01, -4.7176e-01,  2.3953e+00,  6.3145e-01,\n",
       "          -1.1811e-01,  9.7356e-01, -5.4424e-01,  1.4349e+00, -1.0267e-01],\n",
       "         [-7.5313e-01,  3.1157e-01, -9.5696e-01, -8.7168e-01,  9.5688e-01,\n",
       "           1.2971e+00, -7.2866e-01,  5.9098e-01,  3.6317e-01, -1.0567e+00],\n",
       "         [ 8.6752e-01, -6.6223e-01, -3.9630e-01, -3.1225e-01,  1.6617e+00,\n",
       "           1.0858e+00, -5.1975e-01,  5.7400e-01,  1.1734e+00, -5.6563e-01],\n",
       "         [-9.7207e-01,  1.2991e+00,  1.4863e+00,  1.5902e+00,  2.5751e-01,\n",
       "           3.3310e-01,  5.9129e-01, -8.9712e-01, -3.3197e-01,  3.6945e-01],\n",
       "         [ 1.0776e+00,  6.1446e-01,  9.7931e-02, -4.1962e-02, -2.2843e-01,\n",
       "          -1.2041e+00, -1.3627e+00, -8.5267e-01, -3.6945e-01, -8.4783e-01],\n",
       "         [ 1.8694e+00, -1.9977e+00, -3.1920e-01, -7.0429e-01, -5.9238e-01,\n",
       "           6.9186e-01,  7.1262e-01, -6.5669e-01, -1.8861e+00, -5.4974e-01],\n",
       "         [-2.6553e+00, -2.2542e-02, -1.4566e-01, -1.5385e-01,  5.2163e-02,\n",
       "           1.3590e+00, -2.1451e+00,  1.3587e-01,  3.9973e-01,  6.8645e-01],\n",
       "         [ 5.7762e-01, -5.7249e-01, -1.5509e+00, -1.4757e+00, -8.2638e-01,\n",
       "           5.9124e-01,  1.1709e+00, -9.5151e-01, -1.6212e+00,  1.2168e+00],\n",
       "         [ 1.8433e-01,  8.4184e-01, -8.0632e-01, -6.0578e-01,  8.5059e-01,\n",
       "           6.6845e-02,  3.5656e-01, -3.9543e-01, -6.7881e-01, -1.4558e-01],\n",
       "         [-4.6550e-01, -2.0746e+00, -1.4230e+00,  2.2050e-01,  1.6113e+00,\n",
       "          -1.6548e+00, -8.8590e-01, -7.1953e-01, -1.3553e-01,  4.3998e-01],\n",
       "         [ 2.5405e+00, -2.0367e-02,  9.4612e-02, -6.1573e-01,  2.4515e-01,\n",
       "          -8.4099e-01,  1.1760e+00,  8.9146e-01, -1.2238e+00, -5.2827e-01],\n",
       "         [ 1.3066e+00,  1.5882e+00, -2.8146e-01,  9.7974e-01,  1.6645e-01,\n",
       "           1.6831e+00,  5.4682e-01, -3.5502e-01,  6.0302e-01,  1.4200e+00],\n",
       "         [ 4.7955e-01, -6.5734e-02, -4.3250e-01,  4.3130e-02, -2.3623e-01,\n",
       "          -1.1378e+00, -4.8422e-02,  9.6910e-02, -9.7061e-02,  9.8302e-01],\n",
       "         [-6.8533e-01, -3.9148e-01,  5.7348e-01,  8.1017e-01,  2.8503e+00,\n",
       "          -7.7814e-01,  4.0401e-01, -1.6159e+00,  7.0890e-01,  1.6065e+00],\n",
       "         [-1.8841e-01, -3.1955e-01,  1.6240e+00,  1.0112e+00,  1.0995e+00,\n",
       "           1.3073e+00,  7.8285e-01, -1.0547e+00, -6.5272e-01,  6.4191e-01],\n",
       "         [ 2.9107e-01, -8.8343e-01, -7.9924e-01, -3.5815e-02,  4.2999e-01,\n",
       "          -8.7943e-01,  1.9312e+00,  1.4640e-01,  1.6044e-01, -1.3893e+00],\n",
       "         [-5.3310e-02, -6.1363e-01,  1.5290e+00,  7.8565e-01,  1.3786e+00,\n",
       "          -8.1164e-01, -1.2014e-03,  4.1994e-01,  4.8478e-01,  6.5258e-01],\n",
       "         [-1.6400e+00, -1.3581e+00,  1.8873e-01, -2.1222e+00, -2.4794e-01,\n",
       "           4.8451e-01,  8.4634e-02, -7.9261e-01,  1.8865e+00, -2.3959e-01],\n",
       "         [-5.8644e-01,  3.8660e-01,  3.0334e-02, -4.4445e-01,  1.6527e+00,\n",
       "           2.1526e-01, -9.3284e-01, -5.3323e-01,  1.5710e+00, -3.5359e-01],\n",
       "         [ 4.1612e-01, -5.8296e-02,  6.4417e-01, -1.2358e+00, -1.5337e+00,\n",
       "          -1.4380e+00, -1.6230e+00, -1.0849e+00, -1.5826e-01,  1.8273e-01],\n",
       "         [-2.3044e+00, -3.1844e-01,  1.1147e+00,  1.7104e+00,  2.1233e-01,\n",
       "          -4.4950e-01, -5.8804e-01,  1.3372e+00,  8.3719e-01,  9.7624e-01],\n",
       "         [-8.5198e-01,  4.3985e-02,  1.1343e+00,  1.4809e+00,  2.7415e-01,\n",
       "          -5.8323e-01, -1.0616e+00, -1.0695e-01, -8.3962e-02,  2.2484e+00],\n",
       "         [-1.3606e-01, -5.4818e-01,  8.3797e-02, -9.5439e-01, -5.6939e-01,\n",
       "          -1.1007e+00,  1.9862e-01,  4.4884e-01,  3.3155e-01, -5.3606e-01],\n",
       "         [-7.3863e-01, -7.4396e-01,  7.1118e-01,  7.8348e-01, -8.7412e-01,\n",
       "          -4.8460e-01,  3.8152e-01,  1.5532e+00,  1.3562e-01, -8.1934e-01],\n",
       "         [ 6.4847e-01, -2.8088e+00, -5.3359e-01, -6.8123e-01, -1.2570e+00,\n",
       "           6.1602e-01, -3.7908e-01,  8.1901e-01,  5.6997e-01, -6.9017e-01],\n",
       "         [-3.0589e-01, -9.8453e-01,  2.2172e+00, -1.0324e+00,  1.0090e+00,\n",
       "           7.0409e-01,  1.0077e+00,  3.6025e-01, -7.3262e-01, -3.1956e-02],\n",
       "         [ 8.3434e-01, -5.6551e-02,  1.0059e+00, -2.0080e+00, -7.5981e-01,\n",
       "          -2.7747e+00,  1.2510e-01, -1.3925e+00,  4.3744e-01, -1.4321e+00],\n",
       "         [ 1.3548e-01,  3.8275e-01,  5.5460e-02, -1.6048e+00,  3.5881e-01,\n",
       "           2.1102e+00,  7.2543e-01,  9.7842e-01,  3.6454e-01,  4.2841e-01],\n",
       "         [ 7.7155e-01,  9.9121e-01, -5.7893e-01, -3.0688e-02, -3.8120e-01,\n",
       "           5.4293e-01, -2.4422e-01, -1.1005e+00, -1.5274e+00,  1.0645e+00]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0eecdcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1419.779296875\n",
      "199 15.0370454788208\n",
      "299 0.2749688923358917\n",
      "399 0.006338414270430803\n",
      "499 0.00034584992681629956\n"
     ]
    }
   ],
   "source": [
    "n, input_dim, hidden_dim, output_dim = 64, 784, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(n, input_dim)\n",
    "y = torch.randn(n, output_dim)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(input_dim, hidden_dim, requires_grad = True)\n",
    "w2 = torch.randn(hidden_dim, output_dim, requires_grad = True)\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x @ w1\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu @ w2\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "        \n",
    "        \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    # Update weights using gradient descent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, d_in, d_hidden, d_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.tensor("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
