{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f22d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data.DataLoader as DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transform as transform\n",
    "import torchvision.dataset.ImageFolder as ImageFolder\n",
    "from typing import Tuple, List\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloader(train_dir: str,\n",
    "                      test_dir: str,\n",
    "                      train_transform: transform.Compose | nn.Module = None,\n",
    "                      test_transform: transform.Compose | nn.Module = None,\n",
    "                      batch_size: int, \n",
    "                      num_workers: int = NUM_WORKERS) -> Tuple[DataLoader, DataLoader, List[str]]:\n",
    "    \"\"\" Create train and test DataLoader\n",
    "    Pass train_dir and test_dir directories to get train and validation DataLoader \n",
    "    Args:\n",
    "        train_dir: Path to training directory.\n",
    "        test_dir: Path to testing directory.\n",
    "        transform: torchvision transforms to perform on training and testing data.\n",
    "        batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "        num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "        Where class_names is a list of the target classes.\n",
    "    Example usage:\n",
    "          train_dataloader, test_dataloader, class_names = \\\n",
    "            = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                                 test_dir=path/to/test_dir,\n",
    "                                 transform=some_transform,\n",
    "                                 batch_size=32,\n",
    "                                 num_workers=4)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Create dataset using ImageFolder\n",
    "    \n",
    "    # Train Dataset from train_dir directory\n",
    "    train_dataset = ImageFolder(root=train_dir,\n",
    "                                transform=train_transform,\n",
    "                                target_transform=None)\n",
    "\n",
    "    # Test Dataset from test_dir directory\n",
    "    test_dataset = ImageFolder(root=test_dir,\n",
    "                               transform=test_transform,\n",
    "                               target_transform=None)\n",
    "    \n",
    "    # Get class names of target\n",
    "    class_names = train_dataset.classes\n",
    "    \n",
    "    # Create train, test DataLoader\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True)\n",
    "    \n",
    "    test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True)\n",
    "    \n",
    "    return train_dataloader, test_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53517b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding='same'), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*16*16,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x) \n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x) \n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c19164b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def train_step(model: nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = args.device) -> Dict[str, float]:\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        acc = accuracy_fn(y_pred.argmax(dim = 1), y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "        \n",
    "        if idx % 4 == 0:\n",
    "            print(f\"Trained on {idx * len(X)}/{len(dataloader.dataset)}\")\n",
    "            \n",
    "    total_loss /= len(dataloader)\n",
    "    total_acc /= len(dataloader)\n",
    "    \n",
    "    print(f\"Train loss {total_loss:.4f} | Train accuracy {total_acc:.4f}\")\n",
    "    \n",
    "    return {\"loss_score\": total_loss, \"acc_score\" :total_acc}\n",
    "    \n",
    "    \n",
    "    \n",
    "def test_step(model: nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = args.device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            acc = accuracy_fn(y_pred.argmax(dim = 1), y)\n",
    "            \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            \n",
    "        total_loss /= len(dataloader)\n",
    "        total_acc /= len(dataloader)\n",
    "    \n",
    "    print(f\"Test loss {total_loss:.4f} | Test accuracy {total_acc:.4f}\")\n",
    "    \n",
    "    return {\"loss_score\": total_loss, \"acc_score\": total_acc}\n",
    "\n",
    "\n",
    "\n",
    "def train_loop(model: nn.Module, \n",
    "               train_loader: torch.utils.data.DataLoader,\n",
    "               test_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               epochs: int = args.epochs) -> Dict[str, float]:\n",
    "\n",
    "    result = {\n",
    "        \"loss_train\": [],\n",
    "        \"acc_train\": [],\n",
    "        \"loss_test\": [],\n",
    "        \"acc_test\": []\n",
    "    }\n",
    "    start_train = timer()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"Epoch {epoch}:\\n-----------\\n\")\n",
    "\n",
    "        train_score = train_step(model=model, \n",
    "                   dataloader=train_loader,\n",
    "                   loss_fn=loss_fn,\n",
    "                   accuracy_fn=accuracy_fn,\n",
    "                   optimizer=optimizer)\n",
    "\n",
    "        test_score = test_step(model=model,\n",
    "                  dataloader=test_loader,\n",
    "                  loss_fn=loss_fn,\n",
    "                  accuracy_fn=accuracy_fn)\n",
    "        result[\"loss_train\"].append(train_score[\"loss_score\"].to(\"cpu\").detach().numpy())\n",
    "        result[\"acc_train\"].append(train_score[\"acc_score\"].to(\"cpu\").detach().numpy())\n",
    "        result[\"loss_test\"].append(test_score[\"loss_score\"].to(\"cpu\").detach().numpy())\n",
    "        result[\"acc_test\"].append(test_score[\"acc_score\"].to(\"cpu\").detach().numpy())\n",
    "        \n",
    "    end_train = timer()\n",
    "\n",
    "    time_train = print_train_time(start=start_train,\n",
    "                                     end=end_train)    \n",
    "    result[\"total_time\"] = time_train\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3b939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
