{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b5d051",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e16121",
   "metadata": {},
   "source": [
    "## Setup device agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup device agnostic code\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(description=\"computer vision model argument\")\n",
    "parser.add_argument(\"--disable-cuda\",  type=bool, default=False, help=\"Choose cuda device to train model?\")\n",
    "parser.add_argument(\"--learning-rate\", \"-lr\", type=float, default=.01, help=\"Learning rate\")\n",
    "parser.add_argument(\"--epochs\", \"-e\", type=int, default=40, help=\"Epochs\")\n",
    "parser.add_argument(\"--MODEL-PATH\", type=str, default=\"../../Module/models\", help=\"Model save path\")\n",
    "parser.add_argument('--file', '-f', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.device = None\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = \"cuda\"\n",
    "else:\n",
    "    args.device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff61bd5",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b049bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} has already been downloaded...\")\n",
    "else:\n",
    "    print(f\"Folder {image_path} wasn't founded. Creating one...\")\n",
    "    \n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(image_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        print(\"Downloading pizza_steak_sushi.zip...\")\n",
    "        response = requests.get(url)\n",
    "        f.write(response.content)\n",
    "        \n",
    "    with zipfile.ZipFile(image_path / \"pizza_steak_sushi.zip\") as zip_file:\n",
    "        print(f\"Extracting file...\")\n",
    "        zip_file.extractall(image_path)\n",
    "        \n",
    "    print(f\"Downloaded Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112053fc",
   "metadata": {},
   "source": [
    "## Data Exploding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600da3c3",
   "metadata": {},
   "source": [
    "### Explode shape, Visualize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_path(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        print(f\"There are {len(dirs)} directories and {len(files)} files in {root}\")\n",
    "        \n",
    "walk_through_path(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(image_path.glob(\"*/*/*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open(list(image_path.glob(\"*/*/*.jpg\"))[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b205b6",
   "metadata": {},
   "source": [
    "### Get random image and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2e9d4",
   "metadata": {},
   "source": [
    "1. Get all of the image paths using `pathlib.Path.glob()` to find all of the files ending in .jpg.\n",
    "2. Pick a random image path using Python's `random.choice()`.\n",
    "3. Get the image class name using `pathlib.Path.parent.stem`.\n",
    "4. And since we're working with images, we'll open the random image path using `PIL.Image.open()` (`PIL` stands for **Python Image Library**).\n",
    "5. We'll then show the image and print some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "# Get all image path in every dir and convert it to list\n",
    "image_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "# Get random image\n",
    "random_image = random.choice(image_list)\n",
    "\n",
    "# Get class name\n",
    "class_random_image = random_image.parent.stem\n",
    "\n",
    "img = Image.open(random_image)\n",
    "\n",
    "# Print\n",
    "print(f\"Class name image: {class_random_image}\")\n",
    "print(f\"Root directory: {random_image}\")\n",
    "print(f\"Width x Height: {img.width} x {img.height}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71593e45",
   "metadata": {},
   "source": [
    "## 3. Transforming data\n",
    "Now what if we wanted to load our image data into **PyTorch**?\n",
    "\n",
    "Before we can use our image data with PyTorch we need to:\n",
    "\n",
    "- Turn it into tensors (numerical representations of our images).\n",
    "- Turn it into a `torch.utils.data.Dataset` and subsequently a `torch.utils.data.DataLoader`, we'll call these Dataset and *DataLoader* for short.\n",
    "- There are several different kinds of pre-built datasets and dataset loaders for **PyTorch**, depending on the problem you're working on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f790e",
   "metadata": {},
   "source": [
    "### 3.1 Transforming data with torchvision.transforms\n",
    "We've got folders of images but before we can use them with PyTorch, we need to convert them into tensors.\n",
    "\n",
    "One of the ways we can do this is by using the torchvision.transforms module.\n",
    "\n",
    "`torchvision.transforms` contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for data augmentation (the practice of altering data to make it harder for a model to learn, we'll see this later on) purposes .\n",
    "\n",
    "To get experience with `torchvision.transforms`, let's write a series of transform steps that:\n",
    "\n",
    " - Resize the images using `transforms.Resize()` (from about *512x512* to *64x64*, the same shape as the images on the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/)).\n",
    " - Flip our images randomly on the horizontal using `transforms.RandomHorizontalFlip()` (this could be considered a form of data augmentation because it will artificially change our image data).\n",
    " - Turn our images from a `PIL` image to a PyTorch tensor using `transforms.ToTensor()`.\n",
    " - We can compile all of these steps using `torchvision.transforms.Compose()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "data_transforms = Compose([\n",
    "    Resize(size=(64, 64)),\n",
    "    RandomHorizontalFlip(p=.7),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(82)\n",
    "\n",
    "def plot_transform_image(image_paths, transforms, n=4):\n",
    "    \"\"\"\n",
    "    Plot transform random n images\n",
    "    Print class name and size\n",
    "    \n",
    "    Arg:\n",
    "        - image_paths: path to dataset, this must be Path instance\n",
    "        - transforms: data transform pipeline\n",
    "        - n: number of image that will be ploted\n",
    "    \"\"\"\n",
    "    image_list = list(image_paths.glob(\"*/*/*.jpg\"))\n",
    "    image_random_choice = random.sample(list(image_list), k=n)\n",
    "    for idx, image_path in enumerate(image_random_choice):\n",
    "        with Image.open(image_path) as img:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Original \\nSize: {img.size}\")\n",
    "            ax1.axis(False)\n",
    "            \n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transforms(img).permute(1, 2, 0)\n",
    "            ax2.imshow(transformed_image)\n",
    "            ax2.set_title(f\"Original \\nSize: {transformed_image.shape}\")\n",
    "            ax2.axis(False)\n",
    "            \n",
    "            fig.suptitle(f\"Class name {image_path.parent.stem}\", fontsize=16)\n",
    "    \n",
    "    \n",
    "plot_transform_image(image_path, data_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82619c39",
   "metadata": {},
   "source": [
    "### 4. Turn loaded images into `DataLoader`'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transform = Compose([\n",
    "    Resize(size=(64, 64)),\n",
    "    RandomHorizontalFlip(p=.7),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize(size=(64, 64)),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83434361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dataset = ImageFolder(root=train_dir,\n",
    "                            transform=train_transform,\n",
    "                            target_transform=None)\n",
    "\n",
    "\n",
    "test_dataset = ImageFolder(root=test_dir,\n",
    "            transform=test_transform,\n",
    "            target_transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 2\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a66247",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_to_idx, train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeada38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb6f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image, label = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Shape {image.shape} -> [batch_size, color_channel, width, height] \\nClass {class_names[label]}\")\n",
    "plt.imshow(image.squeeze().permute(1, 2, 0))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b1226",
   "metadata": {},
   "source": [
    "## Loading Image Data with a Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b524f7",
   "metadata": {},
   "source": [
    "To see this in action, let's work towards replicating `torchvision.datasets.ImageFolder()` by subclassing `torch.utils.data.Dataset` (the base class for all Dataset's in PyTorch).\n",
    "\n",
    "We'll start by importing the modules we need:\n",
    "\n",
    "- Python's `os` for dealing with directories (our data is stored in directories).\n",
    "- Python's `pathlib` for dealing with filepaths (each of our images has a unique filepath).\n",
    "- `torch` for all things PyTorch.\n",
    "- `PIL`'s Image class for loading images.\n",
    "- `torch.utils.data.Dataset` to subclass and create our own custom Dataset.\n",
    "- `torchvision.transforms` to turn our images into tensors.\n",
    "- Various types from Python's `typing` module to add type hints to our code.\n",
    "> Note: You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format you'd like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb4325",
   "metadata": {},
   "source": [
    "### Create helper function to get class names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43891b",
   "metadata": {},
   "source": [
    "Let's write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.\n",
    "\n",
    "To do so, we'll:\n",
    "\n",
    "- Get the class names using `os.scandir()` to traverse a target directory (ideally the directory is in standard image classification format).\n",
    "- Raise an error if the class names aren't found (if this happens, there might be something wrong with the directory structure).\n",
    "- Turn the class names into a dictionary of numerical labels, one for each class.\n",
    "\n",
    "Let's see a small example of step 1 before we write the full function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_directory = train_dir\n",
    "print(f\"Target dir {target_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101669b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in os.scandir(target_directory): \n",
    "    print(row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_class_names(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    class_names = sorted([ dirs.name for dirs in os.scandir(directory) if dirs.is_dir() ])\n",
    "    \n",
    "    if not class_names:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}\")\n",
    "        \n",
    "    class_to_idx = {classes: i for i, classes in enumerate(class_names)}\n",
    "        \n",
    "    return class_names, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names, class_to_idx = scan_class_names(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc08d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root: str, transform=None) -> None:\n",
    "        \n",
    "        self.paths = list(pathlib.Path(root).glob(\"*/*.jpg\"))\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.classes, self.class_to_idx = scan_class_names(root)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        img_path = self.paths[index]\n",
    "        return Image.open(img_path)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Image.Image, int]:\n",
    "        \n",
    "        image_load = self.load_image(index)\n",
    "        class_names = self.paths[index].parent.stem\n",
    "        class_idx = self.class_to_idx[class_names]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(image_load), class_idx\n",
    "        return image_load, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_customdataset = CustomDataset(root=train_dir,\n",
    "                                    transform=train_transform)\n",
    "\n",
    "test_customdataset = CustomDataset(root=test_dir,\n",
    "                                    transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_customdataset[0]\n",
    "print(class_names[label])\n",
    "plt.imshow(img.squeeze().permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c806bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_customdataset), len(test_customdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c21c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for equality amongst our custom Dataset and ImageFolder Dataset\n",
    "print((len(train_customdataset) == len(train_dataset)) & (len(test_customdataset) == len(test_dataset)))\n",
    "print(train_customdataset.classes == train_dataset.classes)\n",
    "print(train_customdataset.class_to_idx == train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c47d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_customdataset)\n",
    "sorted(list(test_customdataset), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca358aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_image_customdataset(dataset : Dataset,\n",
    "                                         n : int = 4,\n",
    "                                         seed: int = 82):\n",
    "    random.seed(seed)\n",
    "    size = len(dataset)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx in range(n):\n",
    "        ranint = random.randint(0, size)\n",
    "        image, label = dataset[ranint]\n",
    "        \n",
    "        plt.subplot( (n // 4) + 1 , 4, idx + 1)\n",
    "        plt.imshow(image.squeeze().permute(1, 2, 0))\n",
    "        plt.title(class_names[label])\n",
    "        plt.axis(False)\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_image_customdataset(test_customdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import TrivialAugmentWide\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_transforms = Compose([\n",
    "    Resize(size=(256, 256)),\n",
    "    TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    Resize(size=(256, 256)),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transform_image(image_path, train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccb54c",
   "metadata": {},
   "source": [
    "## Model TinyVGG without data augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_transform = Compose([\n",
    "    Resize(size=(64, 64)),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_simple_dataset = ImageFolder(root=train_dir,\n",
    "                                   transform=simple_transform,\n",
    "                                   target_transform=None)\n",
    "\n",
    "test_simple_dataset = ImageFolder(root=test_dir,\n",
    "                                   transform=simple_transform,\n",
    "                                   target_transform=None)\n",
    "\n",
    "train_simple_dataset, test_simple_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 8\n",
    "\n",
    "train_simple_dataloader = DataLoader(dataset=train_simple_dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           num_workers=num_workers,\n",
    "                           shuffle=True)\n",
    "\n",
    "test_simple_dataloader = DataLoader(dataset=test_simple_dataset, \n",
    "                                    batch_size=batch_size,\n",
    "                                    num_workers=num_workers,\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n",
    "train_simple_dataloader, test_simple_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding='same'), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*16*16,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x) \n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x) \n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_simple_dataset.classes)).to(args.device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_simple_dataloader))\n",
    "\n",
    "print(f\"Size input image: {image.shape}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    infer_res = model_0(image.type(torch.float).to(args.device))\n",
    "\n",
    "    \n",
    "print(f\"Size output result: {infer_res.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4668cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "    \n",
    "summary(model_0, input_size=[1, 3, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = args.device) -> Dict[str, float]:\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss, total_acc = 0, 0\n",
    "    for idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        acc = accuracy_fn(y_pred.argmax(dim = 1), y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "        \n",
    "        if idx % 4 == 0:\n",
    "            print(f\"Trained on {idx * len(X)}/{len(dataloader.dataset)}\")\n",
    "            \n",
    "    total_loss /= len(dataloader)\n",
    "    total_acc /= len(dataloader)\n",
    "    \n",
    "    print(f\"Train loss {total_loss:.4f} | Train accuracy {total_acc:.4f}\")\n",
    "    \n",
    "    return {\"loss_score\": total_loss, \"acc_score\" :total_acc}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea36e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = args.device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            acc = accuracy_fn(y_pred.argmax(dim = 1), y)\n",
    "            \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            \n",
    "        total_loss /= len(dataloader)\n",
    "        total_acc /= len(dataloader)\n",
    "    \n",
    "    print(f\"Test loss {total_loss:.4f} | Test accuracy {total_acc:.4f}\")\n",
    "    \n",
    "    return {\"loss_score\": total_loss, \"acc_score\": total_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    return (y_pred == y_true).sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_v0 = torch.optim.SGD(params=model_0.parameters(), lr=.01)\n",
    "\n",
    "acc_fn = accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def print_train_time(start: float, end: float, device: torch.device = args.device) -> float:\n",
    "    total_time = end - start\n",
    "    print(f\"Total train time on {device}: {total_time:.3f}\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 5\n",
    "\n",
    "epochs = args.epochs\n",
    "\n",
    "start_train = timer()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch}:\\n-----------\\n\")\n",
    "    \n",
    "    train_step(model=model_0, \n",
    "               dataloader=train_simple_dataloader,\n",
    "               loss_fn=loss_fn,\n",
    "               accuracy_fn=acc_fn,\n",
    "               optimizer=optimizer_v0)\n",
    "    \n",
    "    test_step(model=model_0,\n",
    "              dataloader=test_simple_dataloader,\n",
    "              loss_fn=loss_fn,\n",
    "              accuracy_fn=acc_fn)\n",
    "    \n",
    "end_train = timer()\n",
    "   \n",
    "time_train_v0 = print_train_time(start=start_train,\n",
    "                                 end=end_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91928fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
